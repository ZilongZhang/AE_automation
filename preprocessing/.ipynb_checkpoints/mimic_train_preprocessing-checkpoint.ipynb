{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No section: 'ohdsi'\n",
      "Initializing models for term finder...\n",
      "section_tagger_init...\n",
      "Context init...\n",
      "Context init...\n",
      "Segmentation init...\n",
      "Done initializing models for term finder...\n",
      "Initializing models for value extractor...\n",
      "Done initializing models for value extractor...\n",
      "Initializing models for measurement finder...\n",
      "Done initializing models for measurement finder..\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from multiprocessing import Pool,Process\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/home/zilong.zhang1/AE_automation/ClarityNLP/nlp')\n",
    "sys.path.append('/home/zilong.zhang1/AE_automation/ClarityNLP/')\n",
    "import Sentence_tokenizer_from_Clarity as cla_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_csv = '../data/NOTEEVENTS.csv'\n",
    "des_dir = '../data/segmented_whole_notes/'\n",
    "if os.path.exists(des_dir):\n",
    "    cmd = 'rm -R '+des_dir+'/**'\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilong.zhang1/miniconda3/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "note_df = pd.read_csv(src_csv)\n",
    "note_text = note_df['TEXT'].to_list()\n",
    "nlp = spacy.load('en',disable=[\"ner\", \"tagger\"])\n",
    "nlp_ori = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = pickle.load(open('../preprocessing/result/ratio2/hadm_test.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
       "0     174       22532  167853.0  2151-08-04       NaN       NaN   \n",
       "1     175       13702  107527.0  2118-06-14       NaN       NaN   \n",
       "2     176       13702  167118.0  2119-05-25       NaN       NaN   \n",
       "3     177       13702  196489.0  2124-08-18       NaN       NaN   \n",
       "4     178       26880  135453.0  2162-03-25       NaN       NaN   \n",
       "\n",
       "            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
       "0  Discharge summary      Report   NaN      NaN   \n",
       "1  Discharge summary      Report   NaN      NaN   \n",
       "2  Discharge summary      Report   NaN      NaN   \n",
       "3  Discharge summary      Report   NaN      NaN   \n",
       "4  Discharge summary      Report   NaN      NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_ids = note_df['HADM_ID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nursing/other        822497\n",
       "Radiology            522279\n",
       "Nursing              223556\n",
       "ECG                  209051\n",
       "Physician            141624\n",
       "Discharge summary     59652\n",
       "Echo                  45794\n",
       "Respiratory           31739\n",
       "Nutrition              9418\n",
       "General                8301\n",
       "Rehab Services         5431\n",
       "Social Work            2670\n",
       "Case Management         967\n",
       "Pharmacy                103\n",
       "Consult                  98\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_df['CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "note_ds = note_text[:59652]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59652"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(note_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_others = note_text[59652:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATIENT/TEST INFORMATION:\n",
      "Indication: Atrial fibrillation. Cerebrovascular event/TIA.\n",
      "Height: (in) 59\n",
      "Weight (lb): 103\n",
      "BSA (m2): 1.39 m2\n",
      "BP (mm Hg): 101/67\n",
      "HR (bpm): 100\n",
      "Status: Inpatient\n",
      "Date/Time: [**2201-6-19**] at 16:00\n",
      "Test: Portable TTE (Complete)\n",
      "Doppler: Full Doppler and color Doppler\n",
      "Contrast: None\n",
      "Technical Quality: Adequate\n",
      "\n",
      "\n",
      "INTERPRETATION:\n",
      "\n",
      "Findings:\n",
      "\n",
      "This study was compared to the prior study of [**2200-8-21**].\n",
      "\n",
      "\n",
      "LEFT ATRIUM: Marked LA enlargement.\n",
      "\n",
      "RIGHT ATRIUM/INTERATRIAL SEPTUM: Mildly dilated RA. No ASD by 2D or color\n",
      "Doppler.\n",
      "\n",
      "LEFT VENTRICLE: Mild symmetric LVH with normal cavity size and regional/global\n",
      "systolic function (LVEF>55%). No LV mass/thrombus. [Intrinsic LV systolic\n",
      "function likely depressed given the severity of valvular regurgitation.] TDI\n",
      "E/e' >15, suggesting PCWP>18mmHg.\n",
      "\n",
      "RIGHT VENTRICLE: Normal RV chamber size and free wall motion.\n",
      "\n",
      "AORTA: Normal diameter of aorta at the sinus, ascending and arch levels.\n",
      "\n",
      "AORTIC VALVE: Bioprosthetic aortic valve prosthesis (AVR). AVR well seated,\n",
      "normal leaflet/disc motion and transvalvular gradients. No AR.\n",
      "\n",
      "MITRAL VALVE: Mildly thickened mitral valve leaflets. Moderate mitral annular\n",
      "calcification. Mild thickening of mitral valve chordae. Moderate (2+) MR.\n",
      "\n",
      "TRICUSPID VALVE: Mildly thickened tricuspid valve leaflets. Mild [1+] TR. Mild\n",
      "PA systolic hypertension.\n",
      "\n",
      "PULMONIC VALVE/PULMONARY ARTERY: Normal pulmonic valve leaflet. No PS.\n",
      "Physiologic PR.\n",
      "\n",
      "PERICARDIUM: No pericardial effusion.\n",
      "\n",
      "GENERAL COMMENTS: The rhythm appears to be atrial fibrillation.\n",
      "\n",
      "Conclusions:\n",
      "The left atrium is markedly dilated. No atrial septal defect is seen by 2D or\n",
      "color Doppler. There is mild symmetric left ventricular hypertrophy with\n",
      "normal cavity size and regional/global systolic function (LVEF>55%).\n",
      "[Intrinsic left ventricular systolic function is likely more depressed given\n",
      "the severity of valvular regurgitation.] No masses or thrombi are seen in the\n",
      "left ventricle. Tissue Doppler imaging suggests an increased left ventricular\n",
      "filling pressure (PCWP>18mmHg). Right ventricular chamber size and free wall\n",
      "motion are normal. A bioprosthetic aortic valve prosthesis is present. The\n",
      "aortic valve prosthesis appears well seated, with normal leaflet/disc motion\n",
      "and transvalvular gradients. No aortic regurgitation is seen. The mitral valve\n",
      "leaflets are mildly thickened. Moderate (2+) mitral regurgitation is seen. The\n",
      "tricuspid valve leaflets are mildly thickened. There is mild pulmonary artery\n",
      "systolic hypertension. There is no pericardial effusion.\n",
      "\n",
      "IMPRESSION: No cardiac source of embolism seen. Mild symmetric left\n",
      "ventricular hypertrophy with preserved global and regional biventricular\n",
      "systolic function. Normally-functioning aortic valve bioprosthesis. Moderate\n",
      "mitral regurgitation. Mild pulmonary hypertension.\n",
      "\n",
      "Compared with the prior study (images reviewed) of [**2200-8-21**], severely\n",
      "stenotic native aortic valve has been replaced with a bioprosthesis. There has\n",
      "been some regression of LVH. Mitral regurgitation is better appreciated on the\n",
      "current study, but severity is likely similar. Pulmonary pressures are\n",
      "slightly lower today. The other findings are similar.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(note_others[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find all abbrivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the logic in clarity\n",
    "import en_core_web_sm as english_model\n",
    "data = {}\n",
    "data['nlp'] = english_model.load(disable=[\"ner\", \"tagger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "abb_dict = defaultdict(lambda: 0)\n",
    "for text in note_text:\n",
    "    if re.search(r'[a-zA-Z]\\.[a-zA-Z]',text):\n",
    "        abbs = re.findall(r'[a-zA-Z]\\.(?:[a-zA-Z][\\.|\\s])+',text)\n",
    "        for abb in abbs:\n",
    "            abb_dict[abb] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s.R \n",
      "H.o \n",
      "p.m \n",
      "R.N.\n",
      "p.o.b.i.d.\n",
      "i.v\n",
      "\n",
      "T.M.\n",
      "N.P.\n",
      "u.s.\n",
      "T.O \n",
      "q.s \n",
      "F.u \n",
      "C.L.\n",
      "G.U.\n",
      "y.o.m.\n",
      "t.l.\n",
      "r.n.\n",
      "s.s.\n",
      "r.t.\n",
      "C.I \n",
      "D.M.\n",
      "h.p.r.n.\n",
      "b.i.d\n",
      "\n",
      "a.m\n",
      "\n",
      "s.c.\n",
      "P.M.\n",
      "c.w \n",
      "s.L \n",
      "d.R \n",
      "B.M.\n",
      "r.A \n",
      "c.i.\n",
      "c.o.\n",
      "T.V.\n",
      "S.O.\n",
      "h.o \n",
      "S.C \n",
      "t.K \n",
      "t.E.\n",
      "e.d\n",
      "\n",
      "A.G.\n",
      "H.C.\n",
      "a.m.\n",
      "D.T \n",
      "C.O \n",
      "N.H.\n",
      "S.K \n",
      "P.C.\n",
      "e.d \n",
      "p.m\n",
      "\n",
      "p.t.\n",
      "h.s.\n",
      "S.W.\n",
      "T.O.\n",
      "y.o.f \n",
      "e.A.\n",
      "a.m \n",
      "m.d.\n",
      "r.o \n",
      "n.p.o.\n",
      "p.o.t.i.d.\n",
      "B.W.\n",
      "R.T.\n",
      "E.G.\n",
      "E.D \n",
      "v.o.\n",
      "N.P.O.\n",
      "P.M \n",
      "q.a.m.\n",
      "s.T \n",
      "D.L.\n",
      "g.i.\n",
      "s.l \n",
      "h.o.\n",
      "q.h.s.p.r.n.\n",
      "w.o \n",
      "t.d.\n",
      "C.I.\n",
      "t.E\n",
      "\n",
      "O.U.\n",
      "c.p.\n",
      "I.S.\n",
      "r.D \n",
      "N.O.S \n",
      "I.D.\n",
      "P.S \n",
      "c.l.\n",
      "B.I.\n",
      "I.E \n",
      "d.L \n",
      "R.O.\n",
      "T.D.\n",
      "E.C.\n",
      "S.L.\n",
      "o.C \n",
      "R.N.s \n",
      "d.o \n",
      "d.c \n",
      "u.o.\n",
      "d.s \n",
      "O.R \n",
      "y.o.f.\n",
      "N.Y.\n",
      "I.V.\n",
      "p.o\n",
      "\n",
      "d.r \n",
      "y.o M \n",
      "n.s.\n",
      "n.c\n",
      "\n",
      "P.o.\n",
      "e.L \n",
      "m.a.e.\n",
      "b.m.\n",
      "t.E \n",
      "i.v.\n",
      "p.r.n.\n",
      "R.O.S.\n",
      "t.A\n",
      "\n",
      "d.r.\n",
      "p.o.q.h.s.p.r.n.\n",
      "P.T.\n",
      "f.u \n",
      "I.R \n",
      "q.o.d.\n",
      "P.O \n",
      "E.T \n",
      "B.P \n",
      "I.R.\n",
      "P.O.\n",
      "A.M\n",
      "\n",
      "I.J.\n",
      "p.r.\n",
      "q.s.\n",
      "i.e.\n",
      "i.d.\n",
      "i.r.\n",
      "e.t.\n",
      "l.e\n",
      "\n",
      "B.S.\n",
      "M.D \n",
      "h.l.\n",
      "N.O.\n",
      "t.o \n",
      "P.E.\n",
      "P.S.\n",
      "p.o.q.h.s.\n",
      "o.s.\n",
      "t.a \n",
      "e.w.\n",
      "C.T.\n",
      "J.P.\n",
      "y.o.m \n",
      "O.R.\n",
      "M.I.\n",
      "C.A.R.E \n",
      "D.R \n",
      "I.D \n",
      "F.B.\n",
      "s.A \n",
      "o.u.\n",
      "M.D.\n",
      "a.k.a.\n",
      "y.o.F \n",
      "N.B.\n",
      "U.O.\n",
      "s.P.\n",
      "B.I.D \n",
      "H.D.\n",
      "o.r.\n",
      "p.o.q.a.m.\n",
      "y.o F \n",
      "E.W.\n",
      "t.A \n",
      "B.I.D\n",
      "\n",
      "U.O \n",
      "s.c \n",
      "d.A \n",
      "s.A.\n",
      "y.o.M \n",
      "O.K.\n",
      "y.o \n",
      "U.S \n",
      "e.r \n",
      "B.S \n",
      "H.O.\n",
      "s.o.b.\n",
      "N.C.\n",
      "d.o\n",
      "\n",
      "P.R.N.\n",
      "o.k.\n",
      "B.I.D.\n",
      "A.M.\n",
      "r.n \n",
      "s.s \n",
      "q.i.d.p.r.n.\n",
      "H.R \n",
      "d.t \n",
      "S.T \n",
      "D.R.\n",
      "b.i.d.\n",
      "U.S.\n",
      "p.o.q.d.\n",
      "n.p.\n",
      "n.c \n",
      "R.D.\n",
      "q.d.\n",
      "a.c.\n",
      "R.R.\n",
      "t.i.d.\n",
      "s.r \n",
      "I.V \n",
      "h.D.\n",
      "b.s.\n",
      "t.D \n",
      "A.C.E \n",
      "B.L.\n",
      "M.C.\n",
      "n.c.\n",
      "p.o.q.\n",
      "l.A \n",
      "R.K\n",
      "\n",
      "d.o.\n",
      "D.I.C.\n",
      "N.T \n",
      "p.o.\n",
      "I.E.\n",
      "q.h.s.\n",
      "s.p \n",
      "g.u.\n",
      "c.o \n",
      "s.c.b.i.d.\n",
      "q.i.d.\n",
      "p.a.\n",
      "G.I.\n",
      "i.v.q.\n",
      "Y.O.\n",
      "R.O \n",
      "S.O.B.\n",
      "Y.O \n",
      "i.e \n",
      "s.K \n",
      "A.M \n",
      "s.o.\n",
      "A.m \n",
      "e.p.\n",
      "h.d.\n",
      "p.o \n",
      "n.K \n",
      "H.O \n",
      "B.A.\n",
      "P.O.D.\n",
      "u.o \n",
      "H.R.\n",
      "S.S.\n",
      "Q.S.\n",
      "M.F.\n",
      "R.I.\n",
      "I.P.\n",
      "p.m.\n",
      "T.F.\n",
      "e.A \n",
      "R.N \n",
      "O.S.\n",
      "i.e\n",
      "\n",
      "e.a \n",
      "A.m.\n",
      "B.P.\n",
      "I.O \n",
      "i.s.\n",
      "e.g.\n",
      "I.S \n",
      "s.D \n",
      "m.r.g.\n",
      "e.d.\n",
      "p.o.q.i.d.\n",
      "y.o.\n",
      "S.C.\n",
      "q.p.m.\n",
      "p.s.\n",
      "E.R.\n",
      "d.A.\n",
      "N.E.\n",
      "t.A.\n",
      "E.L \n",
      "e.R \n",
      "V.A.C.\n",
      "r.o.s.\n",
      "s.p.\n",
      "i.u.\n",
      "c.t.\n",
      "t.r \n",
      "v.s.\n",
      "N.P.N.\n",
      "P.A.\n",
      "h.o\n",
      "\n",
      "y.A \n",
      "C.O.\n",
      "M.P.H.\n",
      "H.L.\n",
      "p.g.\n",
      "B.O.\n",
      "i.v \n",
      "U.o.\n",
      "c.i \n",
      "S.R \n",
      "O.T.\n",
      "l.s \n",
      "F.R.\n",
      "o.k \n",
      "O.D.\n",
      "o.r \n",
      "n.t \n",
      "t.s \n",
      "e.g \n",
      "b.i.d \n",
      "n.A \n",
      "A.D.\n",
      "o.d.\n",
      "E.D.\n",
      "S.O \n",
      "M.S.\n",
      "p.e.\n",
      "D.L \n",
      "S.P \n",
      "o.t.\n",
      "R.A.\n"
     ]
    }
   ],
   "source": [
    "abb_list = []\n",
    "for key,value in abb_dict.items():\n",
    "    if value >20:\n",
    "        print(key)\n",
    "        abb_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sR ', 'Ho ', 'pm ', 'RN', 'pobid', 'iv\\n', 'TM', 'NP', 'us', 'TO ', 'qs ', 'Fu ', 'CL', 'GU', 'yom', 'tl', 'rn', 'ss', 'rt', 'CI ', 'DM', 'hprn', 'bid\\n', 'am\\n', 'sc', 'PM', 'cw ', 'sL ', 'dR ', 'BM', 'rA ', 'ci', 'co', 'TV', 'SO', 'ho ', 'SC ', 'tK ', 'tE', 'ed\\n', 'AG', 'HC', 'am', 'DT ', 'CO ', 'NH', 'SK ', 'PC', 'ed ', 'pm\\n', 'pt', 'hs', 'SW', 'TO', 'yof ', 'eA', 'am ', 'md', 'ro ', 'npo', 'potid', 'BW', 'RT', 'EG', 'ED ', 'vo', 'NPO', 'PM ', 'qam', 'sT ', 'DL', 'gi', 'sl ', 'ho', 'qhsprn', 'wo ', 'td', 'CI', 'tE\\n', 'OU', 'cp', 'IS', 'rD ', 'NOS ', 'ID', 'PS ', 'cl', 'BI', 'IE ', 'dL ', 'RO', 'TD', 'EC', 'SL', 'oC ', 'RNs ', 'do ', 'dc ', 'uo', 'ds ', 'OR ', 'yof', 'NY', 'IV', 'po\\n', 'dr ', 'yo M ', 'ns', 'nc\\n', 'Po', 'eL ', 'mae', 'bm', 'tE ', 'iv', 'prn', 'ROS', 'tA\\n', 'dr', 'poqhsprn', 'PT', 'fu ', 'IR ', 'qod', 'PO ', 'ET ', 'BP ', 'IR', 'PO', 'AM\\n', 'IJ', 'pr', 'qs', 'ie', 'id', 'ir', 'et', 'le\\n', 'BS', 'MD ', 'hl', 'NO', 'to ', 'PE', 'PS', 'poqhs', 'os', 'ta ', 'ew', 'CT', 'JP', 'yom ', 'OR', 'MI', 'CARE ', 'DR ', 'ID ', 'FB', 'sA ', 'ou', 'MD', 'aka', 'yoF ', 'NB', 'UO', 'sP', 'BID ', 'HD', 'or', 'poqam', 'yo F ', 'EW', 'tA ', 'BID\\n', 'UO ', 'sc ', 'dA ', 'sA', 'yoM ', 'OK', 'yo ', 'US ', 'er ', 'BS ', 'HO', 'sob', 'NC', 'do\\n', 'PRN', 'ok', 'BID', 'AM', 'rn ', 'ss ', 'qidprn', 'HR ', 'dt ', 'ST ', 'DR', 'bid', 'US', 'poqd', 'np', 'nc ', 'RD', 'qd', 'ac', 'RR', 'tid', 'sr ', 'IV ', 'hD', 'bs', 'tD ', 'ACE ', 'BL', 'MC', 'nc', 'poq', 'lA ', 'RK\\n', 'do', 'DIC', 'NT ', 'po', 'IE', 'qhs', 'sp ', 'gu', 'co ', 'scbid', 'qid', 'pa', 'GI', 'ivq', 'YO', 'RO ', 'SOB', 'YO ', 'ie ', 'sK ', 'AM ', 'so', 'Am ', 'ep', 'hd', 'po ', 'nK ', 'HO ', 'BA', 'POD', 'uo ', 'HR', 'SS', 'QS', 'MF', 'RI', 'IP', 'pm', 'TF', 'eA ', 'RN ', 'OS', 'ie\\n', 'ea ', 'Am', 'BP', 'IO ', 'is', 'eg', 'IS ', 'sD ', 'mrg', 'ed', 'poqid', 'yo', 'SC', 'qpm', 'ps', 'ER', 'dA', 'NE', 'tA', 'EL ', 'eR ', 'VAC', 'ros', 'sp', 'iu', 'ct', 'tr ', 'vs', 'NPN', 'PA', 'ho\\n', 'yA ', 'CO', 'MPH', 'HL', 'pg', 'BO', 'iv ', 'Uo', 'ci ', 'SR ', 'OT', 'ls ', 'FR', 'ok ', 'OD', 'or ', 'nt ', 'ts ', 'eg ', 'bid ', 'nA ', 'AD', 'od', 'ED', 'SO ', 'MS', 'pe', 'DL ', 'SP ', 'ot', 'RA']\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "nopoint_abb_list = []\n",
    "for abb in abb_list:\n",
    "    tmp = re.sub('\\.','',abb)\n",
    "    nopoint_abb_list.append(tmp)\n",
    "print(nopoint_abb_list)\n",
    "print(len(nopoint_abb_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(abb_list,open('abb_list.pkl','wb'))\n",
    "pickle.dump(nopoint_abb_list,open('abb_list_rep.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define cleaning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "abb_list = pickle.load(open('abb_list.pkl','rb'))\n",
    "nopoint_abb_list = pickle.load(open('abb_list_rep.pkl','rb'))\n",
    "def _batch_clean(start):\n",
    "    des_filename = des_dir + 'clean_'+ str(start)\n",
    "    #des_file = open(des_filename, 'a')\n",
    "    start = int(start)\n",
    "    local_seg = note_to_process[start:min(len(note_to_process),start+num)]\n",
    "    local_clean_list = []\n",
    "    for text in tqdm(local_seg):\n",
    "        text = re.sub(r'\\s*\\[.*?\\]\\s*',' ',text)\n",
    "        if re.search(r'[a-zA-Z]\\.[a-zA-Z]',text):\n",
    "            for i in range(len(abb_list)):\n",
    "                text = re.sub(abb_list[i],nopoint_abb_list[i],text)\n",
    "        text = re.sub(r'\\n',' ',text)\n",
    "        text = re.sub(r' :',':',text)\n",
    "        #text = re.sub(r':\\w',': ',text)\n",
    "        text = re.sub(r'\\s+',' ',text)\n",
    "        local_clean_list.append(text)\n",
    "    with open(des_filename,'wb') as f:\n",
    "        pickle.dump(local_clean_list,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "des_dir = '../data/clean_whole_notes/'\n",
    "if os.path.exists(des_dir):\n",
    "    cmd = 'rm -R '+des_dir+'/**'\n",
    "    os.system(cmd)\n",
    "else:\n",
    "    cmd = 'mkdir -p ' + des_dir\n",
    "    os.system(cmd)\n",
    "\n",
    "p_num =40\n",
    "#num = int(len(note_text)/p_num)\n",
    "num = int(np.ceil(len(note_text)/p_num))\n",
    "\n",
    "p = Pool(processes = p_num)\n",
    "p.map(_batch_clean,range(0,len(note_text),num))\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean abbreviation, mean while, save the id for discharge summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 7481/50589 [00:01<00:09, 4663.76it/s]s]\n",
      "  3%|▎         | 1313/50589 [00:02<01:10, 699.90it/s]]]]\n",
      "100%|██████████| 50589/50589 [00:02<00:00, 21834.26it/s]\n",
      " 45%|████▌     | 23011/50589 [00:04<00:03, 6915.18it/s]]\n",
      " 15%|█▌        | 7774/50589 [00:08<00:45, 935.91it/s]s]\n",
      "100%|██████████| 50589/50589 [00:08<00:00, 5784.68it/s]\n",
      " 37%|███▋      | 18688/50589 [00:08<00:13, 2330.31it/s]\n",
      " 59%|█████▊    | 29697/50589 [00:08<00:07, 2898.56it/s]\n",
      " 53%|█████▎    | 26874/50589 [00:08<00:06, 3397.56it/s]\n",
      "100%|██████████| 50589/50589 [00:09<00:00, 5588.71it/s]\n",
      " 65%|██████▌   | 33134/50589 [00:09<00:03, 4622.85it/s]\n",
      "100%|██████████| 50589/50589 [00:12<00:00, 4074.12it/s]\n",
      "100%|██████████| 50557/50557 [00:13<00:00, 3878.17it/s]\n",
      " 28%|██▊       | 14234/50589 [00:13<00:27, 1317.86it/s]\n",
      "100%|██████████| 50589/50589 [00:13<00:00, 3810.36it/s]\n",
      "100%|██████████| 50589/50589 [00:13<00:00, 3771.62it/s]\n",
      "100%|██████████| 50589/50589 [00:13<00:00, 3730.46it/s]\n",
      " 67%|██████▋   | 34090/50589 [00:13<00:05, 2889.58it/s]\n",
      "100%|██████████| 50589/50589 [00:13<00:00, 3696.81it/s]\n",
      "100%|██████████| 50589/50589 [00:13<00:00, 3692.35it/s]\n",
      "100%|██████████| 50589/50589 [00:14<00:00, 3401.83it/s]\n",
      "100%|██████████| 50589/50589 [00:15<00:00, 3254.57it/s]\n",
      "100%|██████████| 50589/50589 [00:16<00:00, 3010.04it/s]\n",
      "100%|██████████| 50589/50589 [00:17<00:00, 2918.15it/s]\n",
      "100%|██████████| 50589/50589 [00:18<00:00, 2739.74it/s]\n",
      "100%|██████████| 50589/50589 [00:18<00:00, 2731.42it/s]\n",
      " 97%|█████████▋| 49187/50589 [00:19<00:00, 3062.45it/s]\n",
      "100%|██████████| 50589/50589 [00:19<00:00, 2594.09it/s]\n",
      "100%|██████████| 50589/50589 [00:19<00:00, 2593.64it/s]\n",
      "100%|██████████| 50589/50589 [00:19<00:00, 2563.60it/s]\n",
      "100%|██████████| 50589/50589 [00:20<00:00, 2422.22it/s]\n",
      "100%|██████████| 50589/50589 [00:24<00:00, 2101.44it/s]\n",
      "100%|██████████| 50589/50589 [00:34<00:00, 1475.10it/s]\n",
      "100%|██████████| 50589/50589 [00:40<00:00, 1234.75it/s]\n",
      "100%|██████████| 50589/50589 [00:41<00:00, 1219.33it/s]\n",
      "100%|██████████| 50589/50589 [00:41<00:00, 1211.23it/s]\n",
      "100%|██████████| 50589/50589 [00:42<00:00, 1203.09it/s]\n",
      "100%|██████████| 50589/50589 [00:42<00:00, 1186.75it/s]\n",
      "100%|██████████| 50589/50589 [00:43<00:00, 1169.39it/s]\n",
      "100%|██████████| 50589/50589 [00:44<00:00, 1137.95it/s]\n"
     ]
    }
   ],
   "source": [
    "des_dir = '../data/clean_noDS/'\n",
    "if os.path.exists(des_dir):\n",
    "    cmd = 'rm -R '+des_dir+'/**'\n",
    "    os.system(cmd)\n",
    "else:\n",
    "    cmd = 'mkdir -p ' + des_dir\n",
    "    os.system(cmd)\n",
    "note_to_process = note_others\n",
    "p_num =40\n",
    "#num = int(len(note_text)/p_num)\n",
    "num = int(np.ceil(len(note_to_process)/p_num))\n",
    "\n",
    "p = Pool(processes = p_num)\n",
    "p.map(_batch_clean,range(0,len(note_to_process),num))\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "abb_list = pickle.load(open('abb_list.pkl','rb'))\n",
    "nopoint_abb_list = pickle.load(open('abb_list_rep.pkl','rb'))\n",
    "def _batch_clean_keepID(start):\n",
    "    des_filename = des_dir + 'clean_'+ str(start)\n",
    "    #des_file = open(des_filename, 'a')\n",
    "    start = int(start)\n",
    "    #local_seg = note_to_process[start:min(len(note_to_process),start+num)]\n",
    "    id_seg_dict = {}\n",
    "    for i in tqdm(range(start,min(len(note_to_process),start+num))):\n",
    "        text = note_to_process[i]\n",
    "        text = re.sub(r'\\s*\\[.*?\\]\\s*',' ',text)\n",
    "        if re.search(r'[a-zA-Z]\\.[a-zA-Z]',text):\n",
    "            for abb_ind in range(len(abb_list)):\n",
    "                text = re.sub(abb_list[abb_ind],nopoint_abb_list[abb_ind],text)\n",
    "        text = re.sub(r'\\n',' ',text)\n",
    "        text = re.sub(r' :',':',text)\n",
    "        #text = re.sub(r':\\w',': ',text)\n",
    "        text = re.sub(r'\\s+',' ',text)\n",
    "        id_seg_dict[note_ids[i]] = text\n",
    "    return id_seg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [00:02<00:00, 637.21it/s]\n",
      " 13%|█▎        | 191/1492 [00:02<00:14, 90.43it/s]]]\n",
      " 50%|█████     | 746/1492 [00:06<00:06, 120.82it/s]]\n",
      "100%|██████████| 1492/1492 [00:11<00:00, 126.61it/s]\n",
      " 81%|████████  | 1209/1492 [00:11<00:02, 117.58it/s]\n",
      " 83%|████████▎ | 1235/1492 [00:12<00:02, 103.77it/s]\n",
      " 74%|███████▍  | 1109/1492 [00:11<00:03, 119.75it/s]\n",
      "100%|██████████| 1492/1492 [00:11<00:00, 130.24it/s]\n",
      " 84%|████████▍ | 1260/1492 [00:12<00:02, 114.48it/s]\n",
      "100%|██████████| 1492/1492 [00:12<00:00, 123.67it/s]\n",
      "\n",
      " 89%|████████▉ | 1325/1492 [00:12<00:02, 81.26it/s]]\n",
      " 89%|████████▉ | 1335/1492 [00:12<00:01, 79.21it/s]]\n",
      " 98%|█████████▊| 1464/1492 [00:12<00:00, 124.30it/s]\n",
      " 82%|████████▏ | 1218/1492 [00:12<00:02, 127.37it/s]\n",
      "100%|██████████| 1492/1492 [00:13<00:00, 113.18it/s]\n",
      "100%|██████████| 1492/1492 [00:12<00:00, 117.96it/s]\n",
      " 96%|█████████▌| 1431/1492 [00:12<00:00, 113.13it/s]\n",
      "100%|██████████| 1492/1492 [00:13<00:00, 114.48it/s]\n",
      "100%|██████████| 1492/1492 [00:13<00:00, 108.81it/s]\n",
      "100%|██████████| 1492/1492 [00:13<00:00, 114.17it/s]\n",
      "100%|██████████| 1492/1492 [00:12<00:00, 115.49it/s]\n",
      "100%|██████████| 1492/1492 [00:13<00:00, 112.06it/s]\n",
      "100%|██████████| 1492/1492 [00:13<00:00, 106.71it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 104.97it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 105.07it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 104.94it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 104.86it/s]\n",
      "100%|██████████| 1492/1492 [00:13<00:00, 108.19it/s]\n",
      "100%|██████████| 1492/1492 [00:13<00:00, 107.60it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 103.00it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 102.80it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 106.27it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 104.52it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 100.84it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 106.28it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 100.45it/s]\n",
      "100%|██████████| 1492/1492 [00:14<00:00, 99.53it/s] \n",
      "100%|██████████| 1492/1492 [00:14<00:00, 102.63it/s]\n",
      "100%|██████████| 1492/1492 [00:15<00:00, 98.55it/s] \n"
     ]
    }
   ],
   "source": [
    "des_dir = '../data/clean_DS/'\n",
    "if os.path.exists(des_dir):\n",
    "    cmd = 'rm -R '+des_dir+'/**'\n",
    "    os.system(cmd)\n",
    "else:\n",
    "    cmd = 'mkdir -p ' + des_dir\n",
    "    os.system(cmd)\n",
    "    \n",
    "note_to_process = note_ds\n",
    "p_num =40\n",
    "#num = int(len(note_text)/p_num)\n",
    "num = int(np.ceil(len(note_to_process)/p_num))\n",
    "\n",
    "p = Pool(processes = p_num)\n",
    "id_seg_dicts = p.map(_batch_clean_keepID,range(0,len(note_to_process),num))\n",
    "p.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1492"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_id_seg_dict = id_seg_dicts[0]\n",
    "for i in range(1,len(id_seg_dicts)):\n",
    "    tmp_dict = id_seg_dicts[i]\n",
    "    for k,v in tmp_dict.items():\n",
    "        if k in total_id_seg_dict:\n",
    "            total_id_seg_dict[k] = total_id_seg_dict[k] + '\\n' + v\n",
    "        else:\n",
    "            total_id_seg_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_id_seg_dict[134242.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131072.0\n",
      "131077.0\n",
      "131078.0\n",
      "131094.0\n",
      "131109.0\n",
      "131126.0\n",
      "131128.0\n",
      "131132.0\n",
      "131159.0\n",
      "131169.0\n",
      "131171.0\n",
      "131180.0\n",
      "131186.0\n",
      "131200.0\n",
      "131209.0\n",
      "131230.0\n",
      "131232.0\n",
      "131233.0\n",
      "131234.0\n",
      "131245.0\n",
      "131263.0\n",
      "131265.0\n",
      "131282.0\n",
      "131286.0\n",
      "131297.0\n",
      "131301.0\n",
      "131308.0\n",
      "131309.0\n",
      "131312.0\n",
      "131337.0\n",
      "131339.0\n",
      "131340.0\n",
      "131359.0\n",
      "131363.0\n",
      "131364.0\n",
      "131370.0\n",
      "131371.0\n",
      "131372.0\n",
      "131395.0\n",
      "131406.0\n",
      "131407.0\n",
      "131408.0\n",
      "131411.0\n",
      "131414.0\n",
      "131421.0\n",
      "131457.0\n",
      "131460.0\n",
      "131463.0\n",
      "131473.0\n",
      "131493.0\n",
      "131525.0\n",
      "131530.0\n",
      "131550.0\n",
      "131563.0\n",
      "131567.0\n",
      "131571.0\n",
      "131576.0\n",
      "131578.0\n",
      "131594.0\n",
      "131616.0\n",
      "131647.0\n",
      "131656.0\n",
      "131711.0\n",
      "131733.0\n",
      "131744.0\n",
      "131751.0\n",
      "131758.0\n",
      "131762.0\n",
      "131766.0\n",
      "131774.0\n",
      "131799.0\n",
      "131809.0\n",
      "131815.0\n",
      "131821.0\n",
      "131851.0\n",
      "131861.0\n",
      "131862.0\n",
      "131874.0\n",
      "131906.0\n",
      "131907.0\n",
      "131927.0\n",
      "131931.0\n",
      "131932.0\n",
      "131945.0\n",
      "131955.0\n",
      "131974.0\n",
      "131977.0\n",
      "131992.0\n",
      "131994.0\n",
      "131996.0\n",
      "132006.0\n",
      "132007.0\n",
      "132008.0\n",
      "132020.0\n",
      "132059.0\n",
      "132066.0\n",
      "132070.0\n",
      "132071.0\n",
      "132076.0\n",
      "132081.0\n",
      "132104.0\n",
      "132105.0\n",
      "132106.0\n",
      "132114.0\n",
      "132115.0\n",
      "132164.0\n",
      "132165.0\n",
      "132174.0\n",
      "132175.0\n",
      "132180.0\n",
      "132204.0\n",
      "132205.0\n",
      "132210.0\n",
      "132211.0\n",
      "132215.0\n",
      "132244.0\n",
      "132250.0\n",
      "132255.0\n",
      "132278.0\n",
      "132282.0\n",
      "132283.0\n",
      "132293.0\n",
      "132305.0\n",
      "132310.0\n",
      "132316.0\n",
      "132339.0\n",
      "132342.0\n",
      "132348.0\n",
      "132349.0\n",
      "132362.0\n",
      "132390.0\n",
      "132398.0\n",
      "132411.0\n",
      "132414.0\n",
      "132428.0\n",
      "132431.0\n",
      "132436.0\n",
      "132443.0\n",
      "132450.0\n",
      "132489.0\n",
      "132490.0\n",
      "132509.0\n",
      "132522.0\n",
      "132523.0\n",
      "132530.0\n",
      "132534.0\n",
      "132536.0\n",
      "132541.0\n",
      "132556.0\n",
      "132563.0\n",
      "132582.0\n",
      "132590.0\n",
      "132607.0\n",
      "132612.0\n",
      "132619.0\n",
      "132622.0\n",
      "132650.0\n",
      "132654.0\n",
      "132663.0\n",
      "132664.0\n",
      "132666.0\n",
      "132678.0\n",
      "132679.0\n",
      "132683.0\n",
      "132691.0\n",
      "132694.0\n",
      "132696.0\n",
      "132702.0\n",
      "132718.0\n",
      "132738.0\n",
      "132749.0\n",
      "132764.0\n",
      "132773.0\n",
      "132775.0\n",
      "132781.0\n",
      "132782.0\n",
      "132783.0\n",
      "132785.0\n",
      "132794.0\n",
      "132806.0\n",
      "132818.0\n",
      "132843.0\n",
      "132852.0\n",
      "132860.0\n",
      "132864.0\n",
      "132874.0\n",
      "132875.0\n",
      "132890.0\n",
      "132891.0\n",
      "132894.0\n",
      "132911.0\n",
      "132914.0\n",
      "132936.0\n",
      "132953.0\n",
      "132957.0\n",
      "132967.0\n",
      "132972.0\n",
      "132983.0\n",
      "132988.0\n",
      "133011.0\n",
      "133018.0\n",
      "133038.0\n",
      "133047.0\n",
      "133049.0\n",
      "133082.0\n",
      "133083.0\n",
      "133088.0\n",
      "133102.0\n",
      "133126.0\n",
      "133127.0\n",
      "133142.0\n",
      "133150.0\n",
      "133161.0\n",
      "133164.0\n",
      "133190.0\n",
      "133195.0\n",
      "133209.0\n",
      "133226.0\n",
      "133235.0\n",
      "133242.0\n",
      "133262.0\n",
      "133276.0\n",
      "133277.0\n",
      "133293.0\n",
      "133306.0\n",
      "133321.0\n",
      "133326.0\n",
      "133329.0\n",
      "133332.0\n",
      "133350.0\n",
      "133355.0\n",
      "133356.0\n",
      "133367.0\n",
      "133368.0\n",
      "133370.0\n",
      "133377.0\n",
      "133381.0\n",
      "133392.0\n",
      "133394.0\n",
      "133402.0\n",
      "133403.0\n",
      "133412.0\n",
      "133422.0\n",
      "133440.0\n",
      "133470.0\n",
      "133473.0\n",
      "133476.0\n",
      "133485.0\n",
      "133487.0\n",
      "133529.0\n",
      "133532.0\n",
      "133537.0\n",
      "133541.0\n",
      "133543.0\n",
      "133558.0\n",
      "133559.0\n",
      "133577.0\n",
      "133585.0\n",
      "133597.0\n",
      "133600.0\n",
      "133605.0\n",
      "133613.0\n",
      "133617.0\n",
      "133620.0\n",
      "133654.0\n",
      "133660.0\n",
      "133671.0\n",
      "133673.0\n",
      "133696.0\n",
      "133724.0\n",
      "133737.0\n",
      "133745.0\n",
      "133755.0\n",
      "133774.0\n",
      "133786.0\n",
      "133797.0\n",
      "133818.0\n",
      "133841.0\n",
      "133844.0\n",
      "133857.0\n",
      "133934.0\n",
      "133942.0\n",
      "133943.0\n",
      "133959.0\n",
      "133963.0\n",
      "133970.0\n",
      "133986.0\n",
      "133991.0\n",
      "134006.0\n",
      "134007.0\n",
      "134018.0\n",
      "134051.0\n",
      "134053.0\n",
      "134057.0\n",
      "134071.0\n",
      "134087.0\n",
      "134114.0\n",
      "134130.0\n",
      "134139.0\n",
      "134147.0\n",
      "134152.0\n",
      "134156.0\n",
      "134176.0\n",
      "134187.0\n",
      "134197.0\n",
      "134208.0\n",
      "134209.0\n",
      "134219.0\n",
      "134230.0\n",
      "134242.0\n",
      "134257.0\n",
      "134261.0\n",
      "134272.0\n",
      "134283.0\n",
      "134284.0\n",
      "134285.0\n",
      "134286.0\n",
      "134287.0\n",
      "134291.0\n",
      "134311.0\n",
      "134335.0\n",
      "134375.0\n",
      "134379.0\n",
      "134380.0\n",
      "134390.0\n",
      "134396.0\n",
      "134411.0\n",
      "134425.0\n",
      "134446.0\n",
      "134458.0\n",
      "134464.0\n",
      "134466.0\n",
      "134482.0\n",
      "134486.0\n",
      "134503.0\n",
      "134508.0\n",
      "134512.0\n",
      "134518.0\n",
      "134520.0\n",
      "134534.0\n",
      "134541.0\n",
      "134546.0\n",
      "134550.0\n",
      "134554.0\n",
      "134560.0\n",
      "134562.0\n",
      "134565.0\n",
      "134568.0\n",
      "134569.0\n",
      "134573.0\n",
      "134584.0\n",
      "134607.0\n",
      "134610.0\n",
      "134620.0\n",
      "134622.0\n",
      "134624.0\n",
      "134653.0\n",
      "134662.0\n",
      "134678.0\n",
      "134683.0\n",
      "134684.0\n",
      "134697.0\n",
      "134704.0\n",
      "134719.0\n",
      "134737.0\n",
      "134745.0\n",
      "134750.0\n",
      "134775.0\n",
      "134786.0\n",
      "134789.0\n",
      "134815.0\n",
      "134838.0\n",
      "134843.0\n",
      "134844.0\n",
      "134857.0\n",
      "134865.0\n",
      "134867.0\n",
      "134890.0\n",
      "134907.0\n",
      "134909.0\n",
      "134915.0\n",
      "134916.0\n",
      "134925.0\n",
      "134937.0\n",
      "134966.0\n",
      "134981.0\n",
      "134983.0\n",
      "134988.0\n",
      "134989.0\n",
      "134998.0\n",
      "135007.0\n",
      "135008.0\n",
      "135012.0\n",
      "135039.0\n",
      "135045.0\n",
      "135057.0\n",
      "135064.0\n",
      "135084.0\n",
      "135086.0\n",
      "135088.0\n",
      "135090.0\n",
      "135096.0\n",
      "135111.0\n",
      "135138.0\n",
      "135139.0\n",
      "135144.0\n",
      "135149.0\n",
      "135156.0\n",
      "135195.0\n",
      "135206.0\n",
      "135215.0\n",
      "135230.0\n",
      "135233.0\n",
      "135237.0\n",
      "135239.0\n",
      "135263.0\n",
      "135265.0\n",
      "135268.0\n",
      "135269.0\n",
      "135290.0\n",
      "135297.0\n",
      "135337.0\n",
      "135365.0\n",
      "135373.0\n",
      "135377.0\n",
      "135378.0\n",
      "135385.0\n",
      "135389.0\n",
      "135393.0\n",
      "135397.0\n",
      "135398.0\n",
      "135407.0\n",
      "135416.0\n",
      "135421.0\n",
      "135442.0\n",
      "135444.0\n",
      "135445.0\n",
      "135447.0\n",
      "135454.0\n",
      "135457.0\n",
      "135468.0\n",
      "135470.0\n",
      "135473.0\n",
      "135475.0\n",
      "135480.0\n",
      "135487.0\n",
      "135509.0\n",
      "135557.0\n",
      "135560.0\n",
      "135563.0\n",
      "135568.0\n",
      "135573.0\n",
      "135583.0\n",
      "135603.0\n",
      "135605.0\n",
      "135610.0\n",
      "135612.0\n",
      "135614.0\n",
      "135622.0\n",
      "135635.0\n",
      "135639.0\n",
      "135642.0\n",
      "135643.0\n",
      "135644.0\n",
      "135659.0\n",
      "135668.0\n",
      "135680.0\n",
      "135691.0\n",
      "135699.0\n",
      "135705.0\n",
      "135707.0\n",
      "135714.0\n",
      "135740.0\n",
      "135767.0\n",
      "135773.0\n",
      "135779.0\n",
      "135801.0\n",
      "135806.0\n",
      "135818.0\n",
      "135864.0\n",
      "135880.0\n",
      "135887.0\n",
      "135910.0\n",
      "135917.0\n",
      "135931.0\n",
      "135940.0\n",
      "135945.0\n",
      "135949.0\n",
      "135955.0\n",
      "135962.0\n",
      "135966.0\n",
      "135968.0\n",
      "136006.0\n",
      "136614.0\n",
      "138225.0\n",
      "138393.0\n",
      "139104.0\n",
      "139320.0\n",
      "139354.0\n",
      "139808.0\n",
      "140765.0\n",
      "141317.0\n",
      "141958.0\n",
      "142590.0\n",
      "143195.0\n",
      "143820.0\n",
      "144067.0\n",
      "144804.0\n",
      "145993.0\n",
      "146025.0\n",
      "146338.0\n",
      "146621.0\n",
      "147012.0\n",
      "147257.0\n",
      "147264.0\n",
      "147884.0\n",
      "147928.0\n",
      "148188.0\n",
      "148532.0\n",
      "148720.0\n",
      "149073.0\n",
      "149621.0\n",
      "149669.0\n",
      "150533.0\n",
      "150657.0\n",
      "151245.0\n",
      "151802.0\n",
      "152074.0\n",
      "152287.0\n",
      "152998.0\n",
      "153331.0\n",
      "153353.0\n",
      "153386.0\n",
      "153642.0\n",
      "154087.0\n",
      "154282.0\n",
      "154784.0\n",
      "155315.0\n",
      "155489.0\n",
      "155581.0\n",
      "156564.0\n",
      "156707.0\n",
      "157227.0\n",
      "158278.0\n",
      "158288.0\n",
      "158703.0\n",
      "158925.0\n",
      "159261.0\n",
      "159850.0\n",
      "160054.0\n",
      "160169.0\n",
      "160483.0\n",
      "161013.0\n",
      "162004.0\n",
      "162442.0\n",
      "162449.0\n",
      "162706.0\n",
      "163205.0\n",
      "163447.0\n",
      "164098.0\n",
      "164138.0\n",
      "164601.0\n",
      "164768.0\n",
      "165081.0\n",
      "165230.0\n",
      "165318.0\n",
      "165554.0\n",
      "165898.0\n",
      "165992.0\n",
      "166072.0\n",
      "166103.0\n",
      "166934.0\n",
      "167263.0\n",
      "167381.0\n",
      "168775.0\n",
      "169575.0\n",
      "170039.0\n",
      "170494.0\n",
      "170580.0\n",
      "170704.0\n",
      "172343.0\n",
      "172584.0\n",
      "173348.0\n",
      "173459.0\n",
      "173568.0\n",
      "174179.0\n",
      "174498.0\n",
      "174737.0\n",
      "176121.0\n",
      "176182.0\n",
      "176475.0\n",
      "176834.0\n",
      "177419.0\n",
      "177625.0\n",
      "178348.0\n",
      "178559.0\n",
      "178637.0\n",
      "179772.0\n",
      "179848.0\n",
      "180396.0\n",
      "182822.0\n",
      "182906.0\n",
      "182959.0\n",
      "183034.0\n",
      "183421.0\n",
      "183495.0\n",
      "183679.0\n",
      "184037.0\n",
      "184418.0\n",
      "184721.0\n",
      "184812.0\n",
      "184828.0\n",
      "184970.0\n",
      "185228.0\n",
      "185281.0\n",
      "185610.0\n",
      "186635.0\n",
      "186660.0\n",
      "188047.0\n",
      "188278.0\n",
      "188849.0\n",
      "188906.0\n",
      "189393.0\n",
      "190229.0\n",
      "190261.0\n",
      "190818.0\n",
      "192151.0\n",
      "192193.0\n",
      "192401.0\n",
      "193276.0\n",
      "193364.0\n",
      "193746.0\n",
      "193770.0\n",
      "193985.0\n",
      "194773.0\n",
      "194859.0\n",
      "195581.0\n",
      "196381.0\n",
      "196517.0\n",
      "196874.0\n",
      "197406.0\n",
      "197549.0\n",
      "197576.0\n",
      "198034.0\n",
      "198158.0\n",
      "199292.0\n",
      "199509.0\n",
      "199930.0\n",
      "100286.0\n",
      "100336.0\n",
      "103031.0\n",
      "103298.0\n",
      "103389.0\n",
      "103726.0\n",
      "103879.0\n",
      "103975.0\n",
      "104390.0\n",
      "104834.0\n",
      "105228.0\n",
      "105916.0\n",
      "105954.0\n",
      "107559.0\n",
      "107655.0\n",
      "107755.0\n",
      "108652.0\n",
      "108890.0\n",
      "108976.0\n",
      "109243.0\n",
      "109574.0\n",
      "110314.0\n",
      "110559.0\n",
      "110816.0\n",
      "110824.0\n",
      "111084.0\n",
      "111115.0\n",
      "112940.0\n",
      "112988.0\n",
      "113218.0\n",
      "114005.0\n",
      "114277.0\n",
      "114653.0\n",
      "114803.0\n",
      "114836.0\n",
      "114932.0\n",
      "115120.0\n",
      "115324.0\n",
      "115339.0\n",
      "116552.0\n",
      "116730.0\n",
      "117398.0\n",
      "118519.0\n",
      "118789.0\n",
      "120592.0\n",
      "120894.0\n",
      "121237.0\n",
      "121324.0\n",
      "121431.0\n",
      "121760.0\n",
      "122192.0\n",
      "122413.0\n",
      "122532.0\n",
      "123177.0\n",
      "123233.0\n",
      "123414.0\n",
      "123732.0\n",
      "123829.0\n",
      "124382.0\n",
      "124482.0\n",
      "125102.0\n",
      "125153.0\n",
      "125689.0\n",
      "126259.0\n",
      "126574.0\n",
      "127257.0\n",
      "127889.0\n",
      "128033.0\n",
      "128103.0\n",
      "128774.0\n",
      "129133.0\n",
      "129391.0\n",
      "129990.0\n",
      "130007.0\n"
     ]
    }
   ],
   "source": [
    "clean_DS_list = []\n",
    "des_filename = des_dir + '/clean_DS.pickle'\n",
    "for k,v in total_id_seg_dict.items():\n",
    "    if k not in test_ids:\n",
    "        clean_DS_list.append(v)\n",
    "    else:\n",
    "        print(k)\n",
    "with open(des_filename,'wb') as f:\n",
    "    pickle.dump(clean_DS_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_note_list = []\n",
    "src_clean_dir =  '../data/clean_whole_notes/'\n",
    "for root,_,files in os.walk(src_clean_dir):\n",
    "    for file in files:\n",
    "        f = open(os.path.join(root,file),'rb')\n",
    "        tmp_list = pickle.load(f)\n",
    "        clean_note_list.extend(tmp_list)\n",
    "print(len(clean_note_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(src_clean_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal tokenizer(discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_sent_tokenize(start):\n",
    "    des_filename = des_dir + 'segmented_'+ str(start)\n",
    "    des_file = open(des_filename, 'a')\n",
    "    start = int(start)\n",
    "    local_seg = clean_note_list[start:min(len(clean_note_list),start+num)]\n",
    "    for text in tqdm(local_seg):\n",
    "        bert_text = '\\n'\n",
    "        doc = nlp(text)\n",
    "        to_cancat = ''\n",
    "        for sent in doc.sents:\n",
    "            if len(sent.text.split())< 5 and re.search(r'\\d', sent.text):\n",
    "                to_cancat = sent.text\n",
    "                continue\n",
    "            if not re.search('\\w', sent.text):\n",
    "                continue\n",
    "            bert_text += to_cancat + sent.text+'\\n'\n",
    "            to_cancat = ''\n",
    "        #with open(des_filename, 'a') as f:\n",
    "        des_file.writelines(bert_text)# + ' END_OF_DOC')\n",
    "    des_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#des_dir = '/gpfs/qlong/home/tzzhang/mimicIII/Clarity_segmentation/'\n",
    "des_dir = '../data/Clarity_segmentation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_to_precess = note_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59652"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(note_to_precess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarity tokenizer(discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(cla_token)\n",
    "import pdb\n",
    "def _batch_sent_tokenize_Clarity(start,for_ds=True):\n",
    "    if for_ds:\n",
    "        prefix = 'segmented_DS_'\n",
    "    else:\n",
    "        prefix = 'segmented_others_'\n",
    "    des_filename = des_dir + prefix + str(start)\n",
    "    des_file = open(des_filename, 'a')\n",
    "    start = int(start)\n",
    "    local_seg = note_to_precess[start:min(len(note_to_precess),start+num)]\n",
    "    for text in local_seg:\n",
    "        bert_text = '\\n'\n",
    "        sent_list = cla_token.parse_sentences_spacy(text)\n",
    "        prev_sent = '' \n",
    "        for sent in sent_list:\n",
    "            #sent_text = re.sub(r'\\n',' ',sent.text)+'\\n' \n",
    "            if len(sent.split())< 2 and (re.search(r'[a-zA-Z]\\.[a-zA-Z]',sent)):\n",
    "                prev_sent =prev_sent+' '+sent\n",
    "                #pdb.set_trace()\n",
    "                continue\n",
    "            if prev_sent!='':\n",
    "                bert_text += prev_sent+'\\n'\n",
    "            #if not re.search('\\w', sent):\n",
    "                #sent = ''\n",
    "            prev_sent = sent\n",
    "        #with open(des_filename, 'a') as f:\n",
    "        des_file.writelines(bert_text)# + ' END_OF_DOC')\n",
    "    des_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarity tokenizer with merging to 20 words (current valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(cla_token)\n",
    "import pdb\n",
    "merge_to = 20\n",
    "def _batch_sent_tokenize_Clarity_to20(start,for_ds=True):\n",
    "    if for_ds:\n",
    "        prefix = 'segmented_DS_'\n",
    "    else:\n",
    "        prefix = 'segmented_others_'\n",
    "    des_filename = des_dir + prefix + str(start)\n",
    "    des_file = open(des_filename, 'a')\n",
    "    start = int(start)\n",
    "    local_seg = note_to_precess[start:min(len(note_to_precess),start+num)]\n",
    "    for text in local_seg:\n",
    "        bert_text = '\\n'\n",
    "        sent_list = cla_token.parse_sentences_spacy(text)\n",
    "        prev_sent = '' \n",
    "        buffer = ''\n",
    "        cur_len = 0\n",
    "        for sent in sent_list:\n",
    "            buffer = buffer + sent +' '\n",
    "            cur_len = cur_len + len(sent.split())\n",
    "            if cur_len>=merge_to:\n",
    "                bert_text += buffer + '\\n'\n",
    "                buffer = ''\n",
    "                cur_len = 0\n",
    "        if cur_len>0:\n",
    "            bert_text += buffer + '\\n'\n",
    "        des_file.writelines(bert_text)# + ' END_OF_DOC')\n",
    "    des_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS with testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...Segmentation init...\n",
      "Segmentation init...Segmentation init...\n",
      "\n",
      "Segmentation init...Segmentation init...\n",
      "Segmentation init...Segmentation init...Segmentation init...\n",
      "\n",
      "\n",
      "Segmentation init...\n",
      "Segmentation init...Segmentation init...\n",
      "Segmentation init...Segmentation init...Segmentation init...\n",
      "Segmentation init...\n",
      "\n",
      "\n",
      "Segmentation init...Segmentation init...\n",
      "\n",
      "Segmentation init...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "des_dir = '../data/Clarity_segmentation_DS/'\n",
    "note_to_precess = note_ds\n",
    "if os.path.exists(des_dir):\n",
    "    cmd = 'rm -R '+des_dir+'/'\n",
    "    os.system(cmd)\n",
    "    cmd = 'mkdir -p '+des_dir+'/'\n",
    "    os.system(cmd)\n",
    "else:\n",
    "    cmd = 'mkdir -p '+des_dir+'/'\n",
    "    os.system(cmd)\n",
    "    \n",
    "p_num =40\n",
    "num = int(np.ceil(len(note_to_precess)/p_num))\n",
    "\n",
    "for_ds=True\n",
    "data_pairs = []\n",
    "\n",
    "p = Pool(processes = p_num)\n",
    "p.map(_batch_sent_tokenize_Clarity_to20,range(0,len(note_to_precess),num))\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS without testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0bc9bf668ccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnote_to_precess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnote_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mnote_to_precess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_to_precess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_ids' is not defined"
     ]
    }
   ],
   "source": [
    "des_dir = '../data/Clarity_segmentation_DS_without_testset/'\n",
    "note_to_precess = []\n",
    "for i in range(len(note_ds)):\n",
    "    if not note_ids[i] in test_ids:\n",
    "        note_to_precess.append(note_ds[i])\n",
    "    else:\n",
    "        print(note_ids[i])\n",
    "print(len(note_to_precess))\n",
    "if os.path.exists(des_dir):\n",
    "    cmd = 'rm -R '+des_dir+'/'\n",
    "    os.system(cmd)\n",
    "    cmd = 'mkdir -p '+des_dir+'/'\n",
    "    os.system(cmd)\n",
    "else:\n",
    "    cmd = 'mkdir -p '+des_dir+'/'\n",
    "    os.system(cmd)\n",
    "    \n",
    "p_num =40\n",
    "num = int(np.ceil(len(note_to_precess)/p_num))\n",
    "\n",
    "for_ds=True\n",
    "data_pairs = []\n",
    "\n",
    "p = Pool(processes = p_num)\n",
    "p.map(_batch_sent_tokenize_Clarity_to20,range(0,len(note_to_precess),num))\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(note_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation init...Segmentation init...\n",
      "\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...Segmentation init...\n",
      "\n",
      "Segmentation init...Segmentation init...\n",
      "\n",
      "Segmentation init...Segmentation init...Segmentation init...\n",
      "\n",
      "\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n",
      "Segmentation init...\n"
     ]
    }
   ],
   "source": [
    "des_dir = '../data/Clarity_segmentation_others/'\n",
    "note_to_precess = note_others\n",
    "if os.path.exists(des_dir):\n",
    "    cmd = 'rm -R '+des_dir+'/**'\n",
    "    os.system(cmd)\n",
    "    cmd = 'mkdir -p '+des_dir+'/'\n",
    "    os.system(cmd)\n",
    "else:\n",
    "    cmd = 'mkdir -p '+des_dir+'/'\n",
    "    os.system(cmd)\n",
    "    \n",
    "p_num =20\n",
    "num = int(np.ceil(len(note_to_precess)/p_num))\n",
    "\n",
    "for_ds=False\n",
    "data_pairs = []\n",
    "\n",
    "for i in range(0,len(note_to_precess),num):\n",
    "    p = Process(target=_batch_sent_tokenize_Clarity_to20, args=(i, for_ds))\n",
    "    p.start()\n",
    "p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('aaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "all_seg = json.load(open('./adm_seg_dict.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_list = []\n",
    "count = 0\n",
    "import pdb\n",
    "for k,v in all_seg.items():\n",
    "    #if count > 5000:\n",
    "        #break\n",
    "    for line in v:\n",
    "        #if len(line.split())==0:\n",
    "            #pdb.set_trace()\n",
    "        len_list.append(len(line.split()))\n",
    "    count = count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_merge(all_seg_dict,merge_to = 20):\n",
    "    merged_dict = {}\n",
    "    buffer = ''\n",
    "    cur_len = 0\n",
    "    for k,v in tqdm(all_seg_dict.items()):\n",
    "        tmp_doc = []\n",
    "        for line in v:\n",
    "            buffer = buffer + line\n",
    "            cur_len = cur_len + len(line.split())\n",
    "            if cur_len>=merge_to:\n",
    "                tmp_doc.append(buffer)\n",
    "        merged_dict[k] = tmp_doc\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50563/50563 [00:11<00:00, 4509.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "merge_to = 20\n",
    "merged_dict = {}\n",
    "buffer = ''\n",
    "cur_len = 0\n",
    "for k,v in tqdm(all_seg.items()):\n",
    "    tmp_doc = []\n",
    "    for line in v:\n",
    "        buffer = buffer +' ' + line\n",
    "        cur_len = cur_len + len(line.split())\n",
    "        if cur_len>=merge_to:\n",
    "            tmp_doc.append(buffer)\n",
    "            buffer = ''\n",
    "            cur_len = 0\n",
    "    if cur_len>0:\n",
    "        tmp_doc.append(buffer)\n",
    "    merged_dict[k] = tmp_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
