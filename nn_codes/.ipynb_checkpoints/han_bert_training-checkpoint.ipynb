{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'../tools')\n",
    "from transformers import modeling_bert\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "model_name = 'SSI_Bert'\n",
    "import time\n",
    "import utils\n",
    "import imp\n",
    "#from utils import batch_sent_loader\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(utils)\n",
    "hpara1 = utils.hpara()\n",
    "hpara1.ratio = 2\n",
    "hpara1.batch_size = 4\n",
    "hpara1.word_layers=4\n",
    "hpara1.sent_layers=1\n",
    "hpara1.use_position_embedding = False\n",
    "hpara1.word_lr = 1e-04 #5e-05\n",
    "hpara1.sent_lr = 8e-04 #2e-04\n",
    "hpara1.decay_step = 10\n",
    "hpara1.decay_gamma = 0.5\n",
    "hpara1.max_sent_len = 64\n",
    "hpara1.fix_cls = True\n",
    "hpara1.head_num = 12\n",
    "hpara1.att_decay_step=3\n",
    "hpara1.att_decay_rate=0.5\n",
    "hpara1.max_doc_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpara1.max_doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpara1.use_SSI_Bert = True\n",
    "hpara1.use_narrow = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hpara1.use_SSI_Bert:\n",
    "    pretrain_model_dir = '../BERT/bert/infection_bert_fix_gast'\n",
    "else:\n",
    "    pretrain_model_dir = '../BERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000'\n",
    "vocab_file = os.path.join(pretrain_model_dir,'vocab.txt')\n",
    "bert_config_file = os.path.join(pretrain_model_dir,'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../BERT/bert/infection_bert_fix_gast'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../tools/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imp.reload(utils)\n",
    "training_generator,validation_generator,dummy_generator = utils.load_data_new()\n",
    "tokenizer = utils._load_tf_tokenizer(vocab_file = vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = modeling_bert.BertConfig.from_json_file(bert_config_file)\n",
    "config.num_hidden_layers = hpara1.word_layers\n",
    "config.output_attentions = True\n",
    "model_word = modeling_bert.BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../BERT/bert/infection_bert_fix_gast'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "model_state_dict = pretrain_model_dir + '/pytorch_model.bin'\n",
    "pretrained_dict = torch.load(model_state_dict)\n",
    "model_dict = model_word.state_dict()\n",
    "matched_dict = {}\n",
    "for k in pretrained_dict.keys():\n",
    "    try:\n",
    "        new_k = re.search(r'(bert\\.)(.*)',k).group(2)\n",
    "    except:\n",
    "        continue\n",
    "    if new_k in model_dict:\n",
    "        matched_dict[new_k] = pretrained_dict[k]\n",
    "model_dict.update(matched_dict)\n",
    "model_word.load_state_dict(model_dict)\n",
    "cls_weight = model_word.state_dict()['embeddings.word_embeddings.weight'][101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_doc = modeling_bert.BertConfig.from_json_file(bert_config_file)\n",
    "config_doc.num_hidden_layers = hpara1.sent_layers\n",
    "config_doc.output_attentions = True\n",
    "if hpara1.use_narrow:\n",
    "    config_doc.attention_probs_dropout_prob = 0\n",
    "config_doc.num_attention_heads = hpara1.head_num\n",
    "config_doc.attention_probs_dropout_prob = False\n",
    "use_advanced_loss = hpara1.use_angular\n",
    "use_position_embedding = hpara1.use_position_embedding\n",
    "model_sent = modeling_bert.BertModel_no_embedding_narrow(config_doc,cls_weight,use_position_embedding=use_position_embedding,\\\n",
    "                                                        att_decay_rate = hpara1.att_decay_rate,att_decay_step=hpara1.att_decay_step,att_min_sent=hpara1.att_min_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"att_decay_rate\": 0.5,\n",
       "  \"att_decay_step\": 3,\n",
       "  \"att_min_sent\": 10,\n",
       "  \"attention_probs_dropout_prob\": false,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 1,\n",
       "  \"output_attentions\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28996, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word.state_dict()['embeddings.word_embeddings.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = model_word.cuda()\n",
    "model_sent = model_sent.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(list(time.localtime())[0:3]).replace(', ','_')\n",
    "save_dir = './exp/'+model_name+'_'+date[1:-1]+'_narrow0/'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    cmd = 'mkdir -p ' + save_dir\n",
    "    os.system(cmd)\n",
    "     \n",
    "with open(save_dir + 'hpara.json', 'w') as fp:\n",
    "    json.dump(hpara1.__dict__, fp)\n",
    "    \n",
    "do_train = True\n",
    "do_test = True\n",
    "\n",
    "max_epoch = hpara1.max_epoch\n",
    "log = 'Iter {}/{}, Loss={:.3f},accu={:.3f},time={:.3f}\\n'\n",
    "from tqdm import tqdm\n",
    "batch_size = hpara1.batch_size\n",
    "accumulation_steps = hpara1.accumulation_steps\n",
    "max_sent_len = hpara1.max_sent_len\n",
    "max_doc_len = hpara1.max_doc_len\n",
    "#progress_bar = tqdm(enumerate(training_generator))\n",
    "para_dict = {}\n",
    "hpara_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 0/60, Loss=1974.419,accu=0.649,time=538.211\n",
      "\n",
      "Test Iter 0/60, Loss=459.820,accu=0.666,auc=0.817,time=75.458\n",
      "\n",
      "[[481   0]\n",
      " [241   0]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 1/60, Loss=1650.689,accu=0.720,time=539.755\n",
      "\n",
      "Test Iter 1/60, Loss=351.087,accu=0.784,auc=0.924,time=75.808\n",
      "\n",
      "[[348 133]\n",
      " [ 23 218]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 2/60, Loss=1073.544,accu=0.845,time=535.800\n",
      "\n",
      "Test Iter 2/60, Loss=274.768,accu=0.853,auc=0.939,time=76.906\n",
      "\n",
      "[[404  77]\n",
      " [ 29 212]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 3/60, Loss=1009.801,accu=0.860,time=539.345\n",
      "\n",
      "Test Iter 3/60, Loss=215.072,accu=0.885,auc=0.941,time=75.098\n",
      "\n",
      "[[449  32]\n",
      " [ 51 190]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 4/60, Loss=896.536,accu=0.874,time=538.522\n",
      "\n",
      "Test Iter 4/60, Loss=207.606,accu=0.886,auc=0.951,time=75.617\n",
      "\n",
      "[[431  50]\n",
      " [ 32 209]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 5/60, Loss=790.200,accu=0.887,time=538.285\n",
      "\n",
      "Test Iter 5/60, Loss=206.888,accu=0.885,auc=0.951,time=75.179\n",
      "\n",
      "[[433  48]\n",
      " [ 35 206]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 6/60, Loss=737.293,accu=0.904,time=537.111\n",
      "\n",
      "Test Iter 6/60, Loss=197.891,accu=0.886,auc=0.953,time=75.459\n",
      "\n",
      "[[453  28]\n",
      " [ 54 187]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 7/60, Loss=614.710,accu=0.919,time=537.548\n",
      "\n",
      "Test Iter 7/60, Loss=238.891,accu=0.888,auc=0.952,time=75.672\n",
      "\n",
      "[[452  29]\n",
      " [ 52 189]]\n",
      "0\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "imp.reload(utils)\n",
    "utils.model_train_and_test(hpara1,model_word,model_sent,save_dir,training_generator,validation_generator,tokenizer=tokenizer,narrow=hpara1.use_narrow,start_epoch = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO narrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = modeling_bert.BertConfig.from_json_file(bert_config_file)\n",
    "config.num_hidden_layers = hpara1.word_layers\n",
    "config.output_attentions = True\n",
    "model_word = modeling_bert.BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "model_state_dict = pretrain_model_dir + '/pytorch_model.bin'\n",
    "pretrained_dict = torch.load(model_state_dict)\n",
    "model_dict = model_word.state_dict()\n",
    "matched_dict = {}\n",
    "for k in pretrained_dict.keys():\n",
    "    try:\n",
    "        new_k = re.search(r'(bert\\.)(.*)',k).group(2)\n",
    "    except:\n",
    "        continue\n",
    "    if new_k in model_dict:\n",
    "        matched_dict[new_k] = pretrained_dict[k]\n",
    "model_dict.update(matched_dict)\n",
    "model_word.load_state_dict(model_dict)\n",
    "cls_weight = model_word.state_dict()['embeddings.word_embeddings.weight'][101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_doc = modeling_bert.BertConfig.from_json_file(bert_config_file)\n",
    "config_doc.num_hidden_layers = hpara1.sent_layers\n",
    "config_doc.output_attentions = True\n",
    "config_doc.num_attention_heads = hpara1.head_num\n",
    "use_advanced_loss = hpara1.use_angular\n",
    "use_position_embedding = hpara1.use_position_embedding\n",
    "model_sent = modeling_bert.BertModel_no_embedding(config_doc,cls_weight,use_position_embedding=use_position_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = model_word.cuda()\n",
    "model_sent = model_sent.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(list(time.localtime())[0:3]).replace(', ','_')\n",
    "save_dir = './exp/'+model_name+'_'+date[1:-1]+'/'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    cmd = 'mkdir -p ' + save_dir\n",
    "    os.system(cmd)\n",
    "     \n",
    "with open(save_dir + 'hpara.json', 'w') as fp:\n",
    "    json.dump(hpara1.__dict__, fp)\n",
    "    \n",
    "do_train = True\n",
    "do_test = True\n",
    "\n",
    "max_epoch = hpara1.max_epoch\n",
    "log = 'Iter {}/{}, Loss={:.3f},accu={:.3f},time={:.3f}\\n'\n",
    "from tqdm import tqdm\n",
    "batch_size = hpara1.batch_size\n",
    "accumulation_steps = hpara1.accumulation_steps\n",
    "max_sent_len = hpara1.max_sent_len\n",
    "max_doc_len = hpara1.max_doc_len\n",
    "#progress_bar = tqdm(enumerate(training_generator))\n",
    "para_dict = {}\n",
    "hpara_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 0/60, Loss=2038.013,accu=0.626,time=543.362\n",
      "\n",
      "Test Iter 0/60, Loss=459.916,accu=0.666,auc=0.842,time=77.003\n",
      "\n",
      "[[481   0]\n",
      " [241   0]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 1/60, Loss=1850.396,accu=0.663,time=543.704\n",
      "\n",
      "Test Iter 1/60, Loss=477.681,accu=0.666,auc=0.898,time=76.052\n",
      "\n",
      "[[481   0]\n",
      " [241   0]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 2/60, Loss=1847.696,accu=0.673,time=543.806\n",
      "\n",
      "Test Iter 2/60, Loss=417.768,accu=0.668,auc=0.913,time=76.353\n",
      "\n",
      "[[481   0]\n",
      " [240   1]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 3/60, Loss=1245.460,accu=0.810,time=544.103\n",
      "\n",
      "Test Iter 3/60, Loss=242.114,accu=0.860,auc=0.927,time=76.410\n",
      "\n",
      "[[435  46]\n",
      " [ 55 186]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 4/60, Loss=1007.685,accu=0.857,time=542.456\n",
      "\n",
      "Test Iter 4/60, Loss=215.554,accu=0.881,auc=0.942,time=76.544\n",
      "\n",
      "[[443  38]\n",
      " [ 48 193]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 5/60, Loss=856.981,accu=0.879,time=541.512\n",
      "\n",
      "Test Iter 5/60, Loss=207.993,accu=0.877,auc=0.951,time=76.161\n",
      "\n",
      "[[416  65]\n",
      " [ 24 217]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 6/60, Loss=829.867,accu=0.882,time=541.263\n",
      "\n",
      "Test Iter 6/60, Loss=225.930,accu=0.874,auc=0.952,time=75.852\n",
      "\n",
      "[[411  70]\n",
      " [ 21 220]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 7/60, Loss=732.589,accu=0.898,time=541.482\n",
      "\n",
      "Test Iter 7/60, Loss=190.514,accu=0.903,auc=0.953,time=77.181\n",
      "\n",
      "[[442  39]\n",
      " [ 31 210]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 8/60, Loss=602.906,accu=0.929,time=542.492\n",
      "\n",
      "Test Iter 8/60, Loss=220.329,accu=0.892,auc=0.952,time=76.293\n",
      "\n",
      "[[443  38]\n",
      " [ 40 201]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 9/60, Loss=527.741,accu=0.935,time=542.858\n",
      "\n",
      "Test Iter 9/60, Loss=238.889,accu=0.878,auc=0.950,time=75.890\n",
      "\n",
      "[[426  55]\n",
      " [ 33 208]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 10/60, Loss=418.157,accu=0.953,time=542.103\n",
      "\n",
      "Test Iter 10/60, Loss=227.535,accu=0.884,auc=0.948,time=76.639\n",
      "\n",
      "[[435  46]\n",
      " [ 38 203]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 11/60, Loss=370.164,accu=0.960,time=542.579\n",
      "\n",
      "Test Iter 11/60, Loss=286.812,accu=0.888,auc=0.946,time=76.136\n",
      "\n",
      "[[442  39]\n",
      " [ 42 199]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 12/60, Loss=291.153,accu=0.965,time=541.918\n",
      "\n",
      "Test Iter 12/60, Loss=272.051,accu=0.870,auc=0.944,time=75.931\n",
      "\n",
      "[[427  54]\n",
      " [ 40 201]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 13/60, Loss=236.298,accu=0.975,time=543.768\n",
      "\n",
      "Test Iter 13/60, Loss=309.691,accu=0.859,auc=0.945,time=76.860\n",
      "\n",
      "[[408  73]\n",
      " [ 29 212]]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "Iter 14/60, Loss=223.813,accu=0.975,time=540.812\n",
      "\n",
      "Test Iter 14/60, Loss=377.762,accu=0.838,auc=0.939,time=77.259\n",
      "\n",
      "[[387  94]\n",
      " [ 23 218]]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-185:\n",
      "Process Process-186:\n",
      "Process Process-183:\n",
      "Process Process-184:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/zilong.zhang1/miniconda3/envs/py37/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n"
     ]
    }
   ],
   "source": [
    "imp.reload(utils)\n",
    "utils.model_train_and_test(hpara1,model_word,model_sent,save_dir,training_generator,validation_generator,tokenizer=tokenizer,narrow=hpara1.use_narrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Set up the word embedding model and load the pre-trained parameters\n",
    "#if hpara1.use_PU_Bert:\n",
    "#    model_state_dict = './PU_pretrained_model/save_word_complete0.bin'\n",
    "#else:\n",
    "model_state_dict = pretrain_model_dir + '/pytorch_model.bin'\n",
    "pretrained_dict = torch.load(model_state_dict)\n",
    "model_dict = model_word.state_dict()\n",
    "matched_dict = {}\n",
    "for k in pretrained_dict.keys():\n",
    "    try:\n",
    "        new_k = re.search(r'(bert\\.)(.*)',k).group(2)\n",
    "    except:\n",
    "        continue\n",
    "    if new_k in model_dict:\n",
    "        matched_dict[new_k] = pretrained_dict[k]\n",
    "model_dict.update(matched_dict)\n",
    "model_word.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jargon_sent = 'cellulitis sternal erythema abscess vancomycin laparotomy antibiotic exploratory incision trough washout keflex pressor ercp rehab whipple duodenum linezolid complicate biopsy cholecystectomy osteomyelitis orthopedic debridement dehiscence tibia ileostomy colectomy suppository'\n",
    "raw_tokens = tokenizer.tokenize(jargon_sent)\n",
    "print(len(raw_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
