{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'../tools')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html\n",
    "import random\n",
    "from IPython.core.display import display, HTML\n",
    "import utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import modeling_bert\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import imp\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(utils)\n",
    "hpara1 = utils.hpara()\n",
    "hpara1.ratio = 2\n",
    "hpara1.batch_size = 4\n",
    "hpara1.word_layers=4\n",
    "hpara1.sent_layers=1\n",
    "hpara1.use_position_embedding = False\n",
    "hpara1.word_lr = 1e-04 #5e-05\n",
    "hpara1.sent_lr = 8e-04 #2e-04\n",
    "hpara1.decay_step = 10\n",
    "hpara1.decay_gamma = 0.5\n",
    "hpara1.max_sent_len = 64\n",
    "hpara1.fix_cls = True\n",
    "hpara1.head_num = 12\n",
    "hpara1.att_decay_step=3\n",
    "hpara1.att_decay_rate=0.5\n",
    "hpara1.max_doc_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpara1.use_SSI_Bert = False\n",
    "hpara1.use_narrow = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./exp/best_clinical_bert/hpara.json', 'w') as fp:\n",
    "    json.dump(hpara1.__dict__, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../tools/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if hpara1.use_SSI_Bert:\n",
    "    pretrain_model_dir = '../BERT/bert/infection_bert_fix_gast'\n",
    "else:\n",
    "    pretrain_model_dir = '../BERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000'\n",
    "vocab_file = os.path.join(pretrain_model_dir,'vocab.txt')\n",
    "tokenizer = utils._load_tf_tokenizer(vocab_file = vocab_file)\n",
    "bert_config_file = os.path.join(pretrain_model_dir,'bert_config.json')\n",
    "\n",
    "trained_word_model = './exp/best_clinical_bert/save_word_10.bin' \n",
    "trained_sent_model = './exp/best_clinical_bert/save_sent_10.bin' \n",
    "\n",
    "if hpara1.use_SSI_Bert and 'SSI' not in trained_word_model:\n",
    "    print('ERROR')\n",
    "if not hpara1.use_SSI_Bert and 'SSI'  in trained_word_model:\n",
    "    print('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = modeling_bert.BertConfig.from_json_file(bert_config_file)\n",
    "config.num_hidden_layers = hpara1.word_layers\n",
    "config.output_attentions = True\n",
    "model_word = modeling_bert.BertModel(config)\n",
    "model_word.load_state_dict(torch.load(trained_word_model))\n",
    "cls_weight = model_word.state_dict()['embeddings.word_embeddings.weight'][101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_doc = modeling_bert.BertConfig.from_json_file(bert_config_file)\n",
    "config_doc.num_hidden_layers = hpara1.sent_layers\n",
    "config_doc.output_attentions = True\n",
    "config_doc.num_attention_heads = hpara1.head_num\n",
    "use_position_embedding = hpara1.use_position_embedding\n",
    "model_sent = modeling_bert.BertModel_no_embedding(config_doc,cls_weight,use_position_embedding=use_position_embedding)\n",
    "model_sent.load_state_dict(torch.load(trained_sent_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = model_word.cuda()\n",
    "model_sent = model_sent.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{649, 529, 26, 27, 541, 285, 415, 419, 292, 298, 173, 562, 314, 699, 444, 75, 471, 88, 475, 226, 507}\n"
     ]
    }
   ],
   "source": [
    "_,_,_,label_test = utils.load_raw_data_new()\n",
    "svm_y_pred = np.load('../PU_classifier_training/svm_y_pred.npy')\n",
    "bert_y_pred = np.load('y_hat_clinical_noN.npy')[-len(label_test):]\n",
    "svm_thres = 0.37\n",
    "bert_thres = 0.64\n",
    "diff_svm = (svm_y_pred > svm_thres) - np.array(label_test)\n",
    "diff_bert = (bert_y_pred > bert_thres) - np.array(label_test)\n",
    "fn_review = set(np.where(diff_svm == -1)[0]).intersection(np.where(diff_bert == -1)[0])\n",
    "print(fn_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive examples\n",
    "fp_review = {38, 44, 60, 71, 137, 270, 311, 329, 393, 449, 461, 513, 523, 592, 625, 662, 666, 705}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_key_sent(sent_weight,doc,write_file,index):\n",
    "    log = 'Doc {}: <br />key sentence1: {}, score: {}, <br />key sentence2: {}, score: {}, <br />key sentence3: {}, score: {} <br />'\n",
    "    key_sent_index = np.where(sent_weight[1:-1]>0.001)[0] + 1\n",
    "    key_sent_index = key_sent_index[np.argsort(sent_weight[key_sent_index])].tolist()\n",
    "    doc.append('SEP')\n",
    "    doc.insert(0,'CLS')        \n",
    "    #pdb.set_trace()\n",
    "    if len(key_sent_index) == 0:\n",
    "        key_sent_index = [np.argmax(sent_weight[:])]\n",
    "    if len(key_sent_index) <3:\n",
    "        [key_sent_index.append(key_sent_index[0]) for i in range(3 - len(key_sent_index))]\n",
    "\n",
    "    sent1 = doc[key_sent_index[-1]]\n",
    "    weight1 = sent_weight[key_sent_index[-1]]\n",
    "\n",
    "    sent2 = doc[key_sent_index[-2]]\n",
    "    weight2 = sent_weight[key_sent_index[-2]]\n",
    "\n",
    "    sent3 = doc[key_sent_index[-3]]\n",
    "    weight3 = sent_weight[key_sent_index[-3]]\n",
    "\n",
    "    to_print = log.format(index,sent1,weight1,sent2,weight2,sent3,weight3)\n",
    "    write_file.writelines(to_print)\n",
    "    write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "src_dir = '../preprocessing/result/ratio2/'\n",
    "text_test  = pickle.load(open(src_dir+'merged_text_test.pkl','rb')) \n",
    "label_test = pickle.load(open(src_dir+'label_test.pkl','rb'))\n",
    "##Change here!\n",
    "indices_to_review = fn_review\n",
    "text_to_review = []\n",
    "label_to_review = []\n",
    "for ind in indices_to_review:\n",
    "    text_to_review.append(text_test[ind])\n",
    "    label_to_review.append(label_test[ind])\n",
    "params_all = {'batch_size': 1,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 6}\n",
    "review_set = utils.DocDataset(text_to_review, label_to_review)\n",
    "review_generator = DataLoader(review_set, **params_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(list(time.localtime())[0:3]).replace(', ','_')\n",
    "save_dir = './statis/'+date[1:-1]+'/'\n",
    "\n",
    "    \n",
    "if not os.path.exists(save_dir):\n",
    "    cmd = 'mkdir -p ' + save_dir\n",
    "    os.system(cmd)\n",
    "     \n",
    "with open(save_dir + 'used_model.txt','w') as f:\n",
    "    f.writelines(trained_word_model + '\\n' +trained_sent_model)\n",
    "    \n",
    "with open(save_dir + 'hpara.json', 'w') as fp:\n",
    "    json.dump(hpara1.__dict__, fp)\n",
    "    \n",
    "max_epoch = hpara1.max_epoch\n",
    "log = 'Iter {}/{}, Loss={:.3f},accu={:.3f},time={:.3f}\\n'\n",
    "from tqdm import tqdm\n",
    "batch_size = hpara1.batch_size\n",
    "accumulation_steps = hpara1.accumulation_steps\n",
    "max_sent_len = hpara1.max_sent_len\n",
    "max_doc_len = hpara1.max_doc_len\n",
    "#progress_bar = tqdm(enumerate(training_generator))\n",
    "para_dict = {}\n",
    "hpara_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpara1.max_doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sent.eval()\n",
    "model_word.eval()\n",
    "pred_list = []\n",
    "y_list = []\n",
    "y_hat = []\n",
    "total_num = 0\n",
    "correct = 0\n",
    "max_doc_len = hpara1.max_doc_len\n",
    "batch_size = hpara1.batch_size\n",
    "narrow = hpara1.use_narrow\n",
    "def run_model(doc,label):\n",
    "    label = label.cuda()\n",
    "    batch_count=0\n",
    "    sent_num = 0\n",
    "    end_ind = 0\n",
    "    input_tensors = torch.zeros([1,max_doc_len,hpara1.hidden_size]).cuda()\n",
    "    while end_ind <= len(doc) and end_ind < max_doc_len-1:\n",
    "        batch_sent = utils.batch_sent_loader(doc,batch_size,batch_count,max_doc_len=max_doc_len)\n",
    "        cur_batch_size = len(batch_sent)\n",
    "        sent_num += cur_batch_size\n",
    "        input_ids = torch.zeros(cur_batch_size,max_sent_len).long().cuda()\n",
    "        input_mask = torch.ones(cur_batch_size,max_sent_len).long().cuda()\n",
    "        for i in range(len(batch_sent)):\n",
    "            tmp_ids,tmp_mask= utils.word_tokenize(batch_sent[i][0],max_sent_len,tokenizer)\n",
    "            input_ids[i,:] = torch.tensor(tmp_ids)\n",
    "            input_mask[i,:] = torch.tensor(tmp_mask)\n",
    "        _,tmp_input_tensors,word_att_output = model_word(input_ids,attention_mask=input_mask)\n",
    "        start_ind = batch_count*batch_size+1\n",
    "        end_ind = start_ind + cur_batch_size\n",
    "        input_tensors[0,start_ind:end_ind] = tmp_input_tensors\n",
    "        batch_count += 1\n",
    "\n",
    "    sent_mask = [1]*(end_ind)\n",
    "    while len(sent_mask)<max_doc_len:\n",
    "        sent_mask.append(0)\n",
    "    sent_mask = torch.tensor(sent_mask).unsqueeze(0).cuda()\n",
    "    if narrow:\n",
    "        _,proba,sent_att_output = model_sent(input_tensors,label,attention_mask=sent_mask,epoch=cur_epoch)\n",
    "    else:\n",
    "        _,proba,sent_att_output = model_sent(input_tensors,label,attention_mask=sent_mask)\n",
    "    return sent_att_output,doc,proba,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "max_alpha = 0.6 \n",
    "colored_text = '<span style=\"background-color:rgba({},100,{},{} ); line-height:30px\">{}</span>' \n",
    "highlighted_text = []\n",
    "y_list = []\n",
    "y_hat = []\n",
    "proba = []\n",
    "pred_list= []\n",
    "fp_proba_list = []\n",
    "thres = 0.5\n",
    "count = 0\n",
    "\n",
    "for doc_count,(doc,label) in enumerate(review_generator):\n",
    "    sent_att_output,doc,proba,label = run_model(doc,label)\n",
    "    proba = proba.cpu().detach().numpy()\n",
    "    pred_list.append(proba[0,1]>thres)\n",
    "    is_wrong = (proba[0,1]>thres) - label.cpu().numpy()\n",
    "    sent_weight = utils.One_layer_back_trace(sent_att_output[-1])\n",
    "    sent_weight = sent_weight[0]\n",
    "\n",
    "    max_alpha = 0.1\n",
    "    highlighted_text = []\n",
    "    for i in range(min(299,len(doc))):\n",
    "        sent = doc[i][0]\n",
    "        weight = sent_weight[i+1]\n",
    "\n",
    "        red_value =  weight/ max(sent_weight)*255\n",
    "        blue_value = 255 - red_value\n",
    "        color_intensity = max(max_alpha, weight/ max(sent_weight))\n",
    "        new_colored_text = colored_text.format(red_value,blue_value,color_intensity,html.escape(sent+'\\n'))\n",
    "        highlighted_text.append(new_colored_text)\n",
    "        #highlighted_text.append('<span style=\"background-color:rgba(135,206,250,' + str(weight / max_alpha) + ');\">' + html_escape(sent+'\\n') + '</span>')\n",
    "\n",
    "    highlighted_text = '<br />'.join(highlighted_text)\n",
    "    is_wrong = 1\n",
    "    #display(HTML(highlighted_text))\n",
    "    if (is_wrong == 1):\n",
    "        count +=1\n",
    "        write_file =  open(save_dir + str(count) + '.html','w')\n",
    "        write_key_sent(sent_weight,doc,write_file,count)\n",
    "\n",
    "        fp_proba_list.append(proba[0,1])\n",
    "        print(count)\n",
    "        with open(save_dir + str(count) + '.html','a') as f:\n",
    "            f.writelines(highlighted_text)\n",
    "\n",
    "        #print(proba)\n",
    "        #pdb.set_trace()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5386936"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
