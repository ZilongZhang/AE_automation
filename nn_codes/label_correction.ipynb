{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'../tools')\n",
    "from transformers import modeling_bert\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import utils\n",
    "import imp\n",
    "#from utils import batch_sent_loader\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(utils)\n",
    "hpara1 = utils.hpara()\n",
    "hpara1.ratio = 2\n",
    "hpara1.batch_size = 4\n",
    "hpara1.word_layers=4\n",
    "hpara1.sent_layers=1\n",
    "hpara1.use_position_embedding = False\n",
    "hpara1.word_lr = 1e-04 #5e-05\n",
    "hpara1.sent_lr = 8e-04 #2e-04\n",
    "hpara1.decay_step = 10\n",
    "hpara1.decay_gamma = 0.5\n",
    "hpara1.max_sent_len = 64\n",
    "hpara1.fix_cls = True\n",
    "hpara1.head_num = 12\n",
    "hpara1.att_decay_step=3\n",
    "hpara1.att_decay_rate=0.5\n",
    "hpara1.max_doc_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpara1.use_SSI_Bert = False\n",
    "hpara1.use_narrow = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hpara1.use_SSI_Bert:\n",
    "    pretrain_model_dir = '../BERT/bert/infection_bert_fix_gast'\n",
    "else:\n",
    "    pretrain_model_dir = '../BERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000'\n",
    "vocab_file = os.path.join(pretrain_model_dir,'vocab.txt')\n",
    "bert_config_file = os.path.join(pretrain_model_dir,'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_word_model = './exp/best_clinical_bert/save_word_10.bin' \n",
    "trained_sent_model = './exp/best_clinical_bert/save_sent_10.bin' \n",
    "\n",
    "if hpara1.use_SSI_Bert and 'SSI' not in trained_word_model:\n",
    "    print('ERROR')\n",
    "if not hpara1.use_SSI_Bert and 'SSI'  in trained_word_model:\n",
    "    print('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2887\n",
      "WARNING:tensorflow:From ../tools/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imp.reload(utils)\n",
    "all_generator = utils.load_data_for_correction()\n",
    "tokenizer = utils._load_tf_tokenizer(vocab_file = vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = modeling_bert.BertConfig.from_json_file(bert_config_file)\n",
    "config.num_hidden_layers = hpara1.word_layers\n",
    "config.output_attentions = True\n",
    "model_word = modeling_bert.BertModel(config)\n",
    "model_word.load_state_dict(torch.load(trained_word_model))\n",
    "cls_weight = model_word.state_dict()['embeddings.word_embeddings.weight'][101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_doc = modeling_bert.BertConfig.from_json_file(bert_config_file)\n",
    "config_doc.num_hidden_layers = hpara1.sent_layers\n",
    "config_doc.output_attentions = True\n",
    "config_doc.num_attention_heads = hpara1.head_num\n",
    "use_position_embedding = hpara1.use_position_embedding\n",
    "model_sent = modeling_bert.BertModel_no_embedding(config_doc,cls_weight,use_position_embedding=use_position_embedding)\n",
    "model_sent.load_state_dict(torch.load(trained_sent_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = model_word.cuda()\n",
    "model_sent = model_sent.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(list(time.localtime())[0:3]).replace(', ','_')\n",
    "save_dir = './exp/label_correction'+'_'+date[1:-1]+'/'\n",
    "\n",
    "    \n",
    "if not os.path.exists(save_dir):\n",
    "    cmd = 'mkdir -p ' + save_dir\n",
    "    os.system(cmd)\n",
    "     \n",
    "with open(save_dir + 'used_model.txt','w') as f:\n",
    "    f.writelines(trained_word_model + '\\n' +trained_sent_model)\n",
    "    \n",
    "with open(save_dir + 'hpara.json', 'w') as fp:\n",
    "    json.dump(hpara1.__dict__, fp)\n",
    "    \n",
    "max_epoch = hpara1.max_epoch\n",
    "log = 'Iter {}/{}, Loss={:.3f},accu={:.3f},time={:.3f}\\n'\n",
    "from tqdm import tqdm\n",
    "batch_size = hpara1.batch_size\n",
    "accumulation_steps = hpara1.accumulation_steps\n",
    "max_sent_len = hpara1.max_sent_len\n",
    "max_doc_len = hpara1.max_doc_len\n",
    "#progress_bar = tqdm(enumerate(training_generator))\n",
    "para_dict = {}\n",
    "hpara_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in all_generator:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model_sent.eval()\n",
    "model_word.eval()\n",
    "pred_list = []\n",
    "y_list = []\n",
    "y_hat = []\n",
    "total_num = 0\n",
    "correct = 0\n",
    "max_doc_len = hpara1.max_doc_len\n",
    "batch_size = hpara1.batch_size\n",
    "narrow = hpara1.use_narrow\n",
    "for doc_count,(doc,label) in enumerate(all_generator):\n",
    "    total_num += len(label)\n",
    "    label = label.cuda()\n",
    "    batch_count=0\n",
    "    sent_num = 0\n",
    "    end_ind = 0\n",
    "    input_tensors = torch.zeros([1,max_doc_len,hpara1.hidden_size]).cuda()\n",
    "    while end_ind <= len(doc) and end_ind < max_doc_len-1:\n",
    "        batch_sent = utils.batch_sent_loader(doc,batch_size,batch_count,max_doc_len=max_doc_len)\n",
    "        cur_batch_size = len(batch_sent)\n",
    "        sent_num += cur_batch_size\n",
    "        input_ids = torch.zeros(cur_batch_size,max_sent_len).long().cuda()\n",
    "        input_mask = torch.ones(cur_batch_size,max_sent_len).long().cuda()\n",
    "        for i in range(len(batch_sent)):\n",
    "            tmp_ids,tmp_mask= utils.word_tokenize(batch_sent[i][0],max_sent_len,tokenizer)\n",
    "            input_ids[i,:] = torch.tensor(tmp_ids)\n",
    "            input_mask[i,:] = torch.tensor(tmp_mask)\n",
    "        _,tmp_input_tensors,word_att_output = model_word(input_ids,attention_mask=input_mask)\n",
    "        start_ind = batch_count*batch_size+1\n",
    "        end_ind = start_ind + cur_batch_size\n",
    "        input_tensors[0,start_ind:end_ind] = tmp_input_tensors\n",
    "        batch_count += 1\n",
    "\n",
    "    sent_mask = [1]*(end_ind)\n",
    "    while len(sent_mask)<max_doc_len:\n",
    "        sent_mask.append(0)\n",
    "    sent_mask = torch.tensor(sent_mask).unsqueeze(0).cuda()\n",
    "    if narrow:\n",
    "        _,proba,sent_att_output = model_sent(input_tensors,label,attention_mask=sent_mask,epoch=cur_epoch)\n",
    "    else:\n",
    "        _,proba,sent_att_output = model_sent(input_tensors,label,attention_mask=sent_mask)\n",
    "    pos_socre = np.exp(proba.cpu().detach().numpy())[0,1]\n",
    "    y_hat.append(pos_socre)\n",
    "    _,predicted = torch.max(proba,1)\n",
    "    pred_list.append(predicted.item())\n",
    "    y_list.append(label.item())\n",
    "    correct += (predicted == label).sum()\n",
    "accu = correct.item() / total_num\n",
    "roc_score = sklearn.metrics.roc_auc_score(y_list, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_score = sklearn.metrics.roc_auc_score(y_list, y_hat)\n",
    "print(roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = abs(np.array(y_hat) - np.array(y_list))\n",
    "diff_samples = np.where(diffs>0.5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_point,f1_list = utils.f1_maximize(y_hat,y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba[0] = 0.008936026\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008936026,\n",
       " 0.02250839,\n",
       " 0.97538924,\n",
       " 0.008259573,\n",
       " 0.008397106,\n",
       " 0.007297954,\n",
       " 0.008460839,\n",
       " 0.86550313,\n",
       " 0.0072808634,\n",
       " 0.17914106,\n",
       " 0.009298863,\n",
       " 0.5269546,\n",
       " 0.6166816,\n",
       " 0.025314745,\n",
       " 0.9664755,\n",
       " 0.95886815,\n",
       " 0.9487305,\n",
       " 0.010107186,\n",
       " 0.007063042,\n",
       " 0.95596725,\n",
       " 0.00763698,\n",
       " 0.5083653,\n",
       " 0.011274644,\n",
       " 0.90672815,\n",
       " 0.022633879,\n",
       " 0.09693083,\n",
       " 0.010551327,\n",
       " 0.0069132783,\n",
       " 0.008121635,\n",
       " 0.97569126,\n",
       " 0.020878417,\n",
       " 0.9592958,\n",
       " 0.008801162,\n",
       " 0.9355456,\n",
       " 0.97177887,\n",
       " 0.116798475,\n",
       " 0.42381847,\n",
       " 0.011153811,\n",
       " 0.013475675,\n",
       " 0.01003966,\n",
       " 0.012659069,\n",
       " 0.0118601555,\n",
       " 0.010696197,\n",
       " 0.09369572,\n",
       " 0.9225471,\n",
       " 0.40651754,\n",
       " 0.94240403,\n",
       " 0.028212152,\n",
       " 0.020773992,\n",
       " 0.020119939,\n",
       " 0.011074122,\n",
       " 0.97382146,\n",
       " 0.075239725,\n",
       " 0.016594455,\n",
       " 0.007360006,\n",
       " 0.80729,\n",
       " 0.01089012,\n",
       " 0.13137183,\n",
       " 0.0064464286,\n",
       " 0.94421405,\n",
       " 0.48743325,\n",
       " 0.16402219,\n",
       " 0.96964025,\n",
       " 0.007784955,\n",
       " 0.9666,\n",
       " 0.009955134,\n",
       " 0.012527095,\n",
       " 0.009822639,\n",
       " 0.06484791,\n",
       " 0.40835842,\n",
       " 0.39435998,\n",
       " 0.30451536,\n",
       " 0.95492095,\n",
       " 0.943478,\n",
       " 0.008746169,\n",
       " 0.009280892,\n",
       " 0.9644702,\n",
       " 0.008267977,\n",
       " 0.011091117,\n",
       " 0.009724469,\n",
       " 0.014023976,\n",
       " 0.057369117,\n",
       " 0.9795523,\n",
       " 0.006771428,\n",
       " 0.4148097,\n",
       " 0.011367315,\n",
       " 0.91691905,\n",
       " 0.007813051,\n",
       " 0.9146802,\n",
       " 0.95767844,\n",
       " 0.0073072617,\n",
       " 0.09225364,\n",
       " 0.15413986,\n",
       " 0.0979297,\n",
       " 0.9359151,\n",
       " 0.7907719,\n",
       " 0.014740269,\n",
       " 0.006524747,\n",
       " 0.34668007,\n",
       " 0.26083273,\n",
       " 0.96510226,\n",
       " 0.031730108,\n",
       " 0.9703368,\n",
       " 0.68581706,\n",
       " 0.008152652,\n",
       " 0.013814918,\n",
       " 0.97253937,\n",
       " 0.87108475,\n",
       " 0.08966109,\n",
       " 0.801774,\n",
       " 0.94181734,\n",
       " 0.00985996,\n",
       " 0.96501577,\n",
       " 0.03943039,\n",
       " 0.9086899,\n",
       " 0.96958905,\n",
       " 0.022347244,\n",
       " 0.011855395,\n",
       " 0.013614915,\n",
       " 0.9768286,\n",
       " 0.018597009,\n",
       " 0.009688885,\n",
       " 0.008628084,\n",
       " 0.25083387,\n",
       " 0.00748108,\n",
       " 0.96359986,\n",
       " 0.009749638,\n",
       " 0.9657774,\n",
       " 0.014295536,\n",
       " 0.022246357,\n",
       " 0.9725844,\n",
       " 0.007816442,\n",
       " 0.9231015,\n",
       " 0.041547064,\n",
       " 0.009471977,\n",
       " 0.020833883,\n",
       " 0.97715354,\n",
       " 0.9749894,\n",
       " 0.44654125,\n",
       " 0.012085094,\n",
       " 0.95733064,\n",
       " 0.14251192,\n",
       " 0.012350836,\n",
       " 0.0087183,\n",
       " 0.961874,\n",
       " 0.94771904,\n",
       " 0.9366803,\n",
       " 0.009835167,\n",
       " 0.9733296,\n",
       " 0.010709506,\n",
       " 0.9760265,\n",
       " 0.012345437,\n",
       " 0.92025816,\n",
       " 0.0073514967,\n",
       " 0.008304554,\n",
       " 0.008597866,\n",
       " 0.62832916,\n",
       " 0.96644366,\n",
       " 0.0120026255,\n",
       " 0.006945226,\n",
       " 0.01063922,\n",
       " 0.012697561,\n",
       " 0.007936191,\n",
       " 0.009594517,\n",
       " 0.9784587,\n",
       " 0.02143577,\n",
       " 0.007130196,\n",
       " 0.96772933,\n",
       " 0.95667285,\n",
       " 0.0137417875,\n",
       " 0.9311144,\n",
       " 0.0262119,\n",
       " 0.14739013,\n",
       " 0.16400571,\n",
       " 0.9651651,\n",
       " 0.009480725,\n",
       " 0.51412463,\n",
       " 0.9762515,\n",
       " 0.92735946,\n",
       " 0.3080139,\n",
       " 0.83675176,\n",
       " 0.15478149,\n",
       " 0.26431134,\n",
       " 0.0076510683,\n",
       " 0.94126695,\n",
       " 0.006824667,\n",
       " 0.10654344,\n",
       " 0.81398034,\n",
       " 0.97221553,\n",
       " 0.009448741,\n",
       " 0.9307114,\n",
       " 0.039521072,\n",
       " 0.8514413,\n",
       " 0.008035214,\n",
       " 0.010019196,\n",
       " 0.008421157,\n",
       " 0.97984594,\n",
       " 0.0099841,\n",
       " 0.006376033,\n",
       " 0.00993382,\n",
       " 0.0071529537,\n",
       " 0.017520119,\n",
       " 0.008778362,\n",
       " 0.0074318433,\n",
       " 0.011629894,\n",
       " 0.009885028,\n",
       " 0.9392709,\n",
       " 0.7724486,\n",
       " 0.007821356,\n",
       " 0.4100373,\n",
       " 0.008380693,\n",
       " 0.9772599,\n",
       " 0.008883125,\n",
       " 0.009589687,\n",
       " 0.9726556,\n",
       " 0.026344178,\n",
       " 0.25202698,\n",
       " 0.565453,\n",
       " 0.42164358,\n",
       " 0.015882732,\n",
       " 0.59825575,\n",
       " 0.9646288,\n",
       " 0.025964927,\n",
       " 0.008248568,\n",
       " 0.9738095,\n",
       " 0.03700847,\n",
       " 0.9788643,\n",
       " 0.9571773,\n",
       " 0.9767933,\n",
       " 0.9213069,\n",
       " 0.91969347,\n",
       " 0.93090373,\n",
       " 0.08385994,\n",
       " 0.1190303,\n",
       " 0.9573051,\n",
       " 0.9769124,\n",
       " 0.88407886,\n",
       " 0.012191756,\n",
       " 0.9663789,\n",
       " 0.95999277,\n",
       " 0.009465588,\n",
       " 0.97225994,\n",
       " 0.97663546,\n",
       " 0.025652602,\n",
       " 0.47042778,\n",
       " 0.014746273,\n",
       " 0.0119957365,\n",
       " 0.96337456,\n",
       " 0.97067225,\n",
       " 0.011495602,\n",
       " 0.8432082,\n",
       " 0.9733758,\n",
       " 0.0072223432,\n",
       " 0.9586967,\n",
       " 0.32633188,\n",
       " 0.80544573,\n",
       " 0.9554686,\n",
       " 0.9758487,\n",
       " 0.024246665,\n",
       " 0.9578759,\n",
       " 0.97516984,\n",
       " 0.040277462,\n",
       " 0.008321222,\n",
       " 0.009271745,\n",
       " 0.9366306,\n",
       " 0.9776068,\n",
       " 0.00949007,\n",
       " 0.008487027,\n",
       " 0.04097962,\n",
       " 0.007924435,\n",
       " 0.40191108,\n",
       " 0.008772528,\n",
       " 0.9703076,\n",
       " 0.010278131,\n",
       " 0.26058108,\n",
       " 0.011662003,\n",
       " 0.019395329,\n",
       " 0.012525268,\n",
       " 0.007561844,\n",
       " 0.77565086,\n",
       " 0.04465396,\n",
       " 0.018460052,\n",
       " 0.027275339,\n",
       " 0.4109817,\n",
       " 0.0129911825,\n",
       " 0.007960326,\n",
       " 0.14372094,\n",
       " 0.045632,\n",
       " 0.011918907,\n",
       " 0.7284673,\n",
       " 0.01634456,\n",
       " 0.16332981,\n",
       " 0.96850026,\n",
       " 0.007717701,\n",
       " 0.97444624,\n",
       " 0.010569732,\n",
       " 0.008876672,\n",
       " 0.6382696,\n",
       " 0.031921443,\n",
       " 0.04221631,\n",
       " 0.9355673,\n",
       " 0.9350463,\n",
       " 0.107097365,\n",
       " 0.9380756,\n",
       " 0.012641804,\n",
       " 0.94007766,\n",
       " 0.9638556,\n",
       " 0.9681549,\n",
       " 0.05158252,\n",
       " 0.11560084,\n",
       " 0.9654529,\n",
       " 0.12705925,\n",
       " 0.03405224,\n",
       " 0.9384002,\n",
       " 0.028683962,\n",
       " 0.008393492,\n",
       " 0.95867264,\n",
       " 0.96937156,\n",
       " 0.70842284,\n",
       " 0.97434664,\n",
       " 0.97667044,\n",
       " 0.008562173,\n",
       " 0.00798538,\n",
       " 0.020679798,\n",
       " 0.04827084,\n",
       " 0.51452947,\n",
       " 0.909432,\n",
       " 0.0117230425,\n",
       " 0.95212954,\n",
       " 0.9612693,\n",
       " 0.0074716113,\n",
       " 0.011080254,\n",
       " 0.97607213,\n",
       " 0.0582048,\n",
       " 0.4756852,\n",
       " 0.19500467,\n",
       " 0.008765616,\n",
       " 0.09061282,\n",
       " 0.40539613,\n",
       " 0.03307041,\n",
       " 0.009939188,\n",
       " 0.011381363,\n",
       " 0.38232836,\n",
       " 0.012672718,\n",
       " 0.8663959,\n",
       " 0.015292734,\n",
       " 0.029020939,\n",
       " 0.012142042,\n",
       " 0.015423042,\n",
       " 0.006421878,\n",
       " 0.012215795,\n",
       " 0.020147689,\n",
       " 0.01116407,\n",
       " 0.009107752,\n",
       " 0.027284484,\n",
       " 0.011148504,\n",
       " 0.18694007,\n",
       " 0.9700108,\n",
       " 0.028941043,\n",
       " 0.97838825,\n",
       " 0.007902602,\n",
       " 0.015536878,\n",
       " 0.94578254,\n",
       " 0.04013761,\n",
       " 0.010023545,\n",
       " 0.98054814,\n",
       " 0.9626486,\n",
       " 0.008295941,\n",
       " 0.91097474,\n",
       " 0.016922541,\n",
       " 0.93911254,\n",
       " 0.889731,\n",
       " 0.70800674,\n",
       " 0.88820577,\n",
       " 0.011172015,\n",
       " 0.902469,\n",
       " 0.0065279924,\n",
       " 0.7451483,\n",
       " 0.1786163,\n",
       " 0.9761513,\n",
       " 0.91184306,\n",
       " 0.97129154,\n",
       " 0.022294834,\n",
       " 0.9669173,\n",
       " 0.9656333,\n",
       " 0.007706709,\n",
       " 0.007400773,\n",
       " 0.008987764,\n",
       " 0.95747226,\n",
       " 0.07511877,\n",
       " 0.8437546,\n",
       " 0.029063009,\n",
       " 0.9671701,\n",
       " 0.008500798,\n",
       " 0.93333304,\n",
       " 0.2591371,\n",
       " 0.2864487,\n",
       " 0.946345,\n",
       " 0.018618453,\n",
       " 0.8394338,\n",
       " 0.007703006,\n",
       " 0.9275559,\n",
       " 0.012617372,\n",
       " 0.12719104,\n",
       " 0.007450411,\n",
       " 0.8425564,\n",
       " 0.9779889,\n",
       " 0.8542731,\n",
       " 0.89085937,\n",
       " 0.006781794,\n",
       " 0.92884886,\n",
       " 0.010699634,\n",
       " 0.97914106,\n",
       " 0.9690183,\n",
       " 0.00888194,\n",
       " 0.02292762,\n",
       " 0.9525822,\n",
       " 0.7196652,\n",
       " 0.9593854,\n",
       " 0.96993256,\n",
       " 0.96459156,\n",
       " 0.024150593,\n",
       " 0.009903933,\n",
       " 0.9612737,\n",
       " 0.9482381,\n",
       " 0.06359876,\n",
       " 0.95022166,\n",
       " 0.01551688,\n",
       " 0.014113938,\n",
       " 0.9724773,\n",
       " 0.9669408,\n",
       " 0.9707949,\n",
       " 0.8572947,\n",
       " 0.088284284,\n",
       " 0.006856356,\n",
       " 0.01720701,\n",
       " 0.029962614,\n",
       " 0.96205604,\n",
       " 0.042665023,\n",
       " 0.0068086945,\n",
       " 0.038128737,\n",
       " 0.024548514,\n",
       " 0.9270069,\n",
       " 0.96217597,\n",
       " 0.009659794,\n",
       " 0.11001477,\n",
       " 0.9751922,\n",
       " 0.010405928,\n",
       " 0.009055483,\n",
       " 0.009712346,\n",
       " 0.95156115,\n",
       " 0.96073836,\n",
       " 0.9582905,\n",
       " 0.013542185,\n",
       " 0.012940393,\n",
       " 0.006527062,\n",
       " 0.95723855,\n",
       " 0.006637807,\n",
       " 0.008480793,\n",
       " 0.97657245,\n",
       " 0.020312125,\n",
       " 0.7240012,\n",
       " 0.8157377,\n",
       " 0.95486486,\n",
       " 0.012558866,\n",
       " 0.00799707,\n",
       " 0.0069087665,\n",
       " 0.009788894,\n",
       " 0.9658589,\n",
       " 0.961424,\n",
       " 0.9178807,\n",
       " 0.8834265,\n",
       " 0.97273797,\n",
       " 0.092074275,\n",
       " 0.90612733,\n",
       " 0.19292927,\n",
       " 0.10483083,\n",
       " 0.03148256,\n",
       " 0.77481216,\n",
       " 0.17647412,\n",
       " 0.00828594,\n",
       " 0.0063060215,\n",
       " 0.28351095,\n",
       " 0.94445544,\n",
       " 0.2889514,\n",
       " 0.009132453,\n",
       " 0.011887134,\n",
       " 0.9715252,\n",
       " 0.48706555,\n",
       " 0.9783351,\n",
       " 0.9639056,\n",
       " 0.019816931,\n",
       " 0.15193518,\n",
       " 0.019309891,\n",
       " 0.00680333,\n",
       " 0.11718714,\n",
       " 0.9433406,\n",
       " 0.010238602,\n",
       " 0.9390339,\n",
       " 0.15122768,\n",
       " 0.012258637,\n",
       " 0.010166921,\n",
       " 0.9722705,\n",
       " 0.9743643,\n",
       " 0.009585129,\n",
       " 0.0135514345,\n",
       " 0.008170951,\n",
       " 0.014573278,\n",
       " 0.41380095,\n",
       " 0.75195146,\n",
       " 0.0074649733,\n",
       " 0.93147945,\n",
       " 0.97846997,\n",
       " 0.018196292,\n",
       " 0.010008176,\n",
       " 0.5461681,\n",
       " 0.013125266,\n",
       " 0.97703254,\n",
       " 0.034257628,\n",
       " 0.9676732,\n",
       " 0.044290077,\n",
       " 0.8306754,\n",
       " 0.24333586,\n",
       " 0.15546978,\n",
       " 0.043717396,\n",
       " 0.011365007,\n",
       " 0.9344141,\n",
       " 0.977029,\n",
       " 0.97613573,\n",
       " 0.0074173105,\n",
       " 0.0073852087,\n",
       " 0.9709781,\n",
       " 0.04442663,\n",
       " 0.95907795,\n",
       " 0.008786381,\n",
       " 0.013414769,\n",
       " 0.007855964,\n",
       " 0.015247691,\n",
       " 0.009353852,\n",
       " 0.97007465,\n",
       " 0.023635902,\n",
       " 0.0065592076,\n",
       " 0.94676644,\n",
       " 0.9383896,\n",
       " 0.0064046,\n",
       " 0.13569799,\n",
       " 0.89667195,\n",
       " 0.027212346,\n",
       " 0.15597707,\n",
       " 0.9276909,\n",
       " 0.075423226,\n",
       " 0.013191857,\n",
       " 0.96071446,\n",
       " 0.54451,\n",
       " 0.008850468,\n",
       " 0.9802664,\n",
       " 0.026972847,\n",
       " 0.010173749,\n",
       " 0.9720976,\n",
       " 0.92752856,\n",
       " 0.0079328045,\n",
       " 0.009425624,\n",
       " 0.008108656,\n",
       " 0.008808135,\n",
       " 0.011891896,\n",
       " 0.97159034,\n",
       " 0.17453325,\n",
       " 0.43425208,\n",
       " 0.009787475,\n",
       " 0.08649934,\n",
       " 0.008468476,\n",
       " 0.95638937,\n",
       " 0.024632381,\n",
       " 0.96093863,\n",
       " 0.2805217,\n",
       " 0.018669056,\n",
       " 0.96996516,\n",
       " 0.009255868,\n",
       " 0.89431524,\n",
       " 0.95616496,\n",
       " 0.008950283,\n",
       " 0.0913614,\n",
       " 0.019947644,\n",
       " 0.00801952,\n",
       " 0.6297942,\n",
       " 0.0072747553,\n",
       " 0.8786606,\n",
       " 0.21009558,\n",
       " 0.010380143,\n",
       " 0.069070116,\n",
       " 0.93692005,\n",
       " 0.01882838,\n",
       " 0.04019075,\n",
       " 0.008804407,\n",
       " 0.010887565,\n",
       " 0.9612376,\n",
       " 0.97238153,\n",
       " 0.97278625,\n",
       " 0.96902543,\n",
       " 0.8039127,\n",
       " 0.74943733,\n",
       " 0.31242245,\n",
       " 0.014414701,\n",
       " 0.0310943,\n",
       " 0.013905316,\n",
       " 0.81577516,\n",
       " 0.013016743,\n",
       " 0.96622646,\n",
       " 0.014281126,\n",
       " 0.432064,\n",
       " 0.93945545,\n",
       " 0.95837843,\n",
       " 0.030186608,\n",
       " 0.9765944,\n",
       " 0.01396739,\n",
       " 0.11436786,\n",
       " 0.95954365,\n",
       " 0.013575002,\n",
       " 0.0066455505,\n",
       " 0.010631085,\n",
       " 0.9070606,\n",
       " 0.20186777,\n",
       " 0.01396713,\n",
       " 0.0074979938,\n",
       " 0.31934917,\n",
       " 0.9689239,\n",
       " 0.6936288,\n",
       " 0.53504,\n",
       " 0.948849,\n",
       " 0.010210724,\n",
       " 0.010631278,\n",
       " 0.1864101,\n",
       " 0.10703534,\n",
       " 0.97624516,\n",
       " 0.013233894,\n",
       " 0.97086114,\n",
       " 0.96189606,\n",
       " 0.70585525,\n",
       " 0.9725831,\n",
       " 0.018560974,\n",
       " 0.9788504,\n",
       " 0.008436209,\n",
       " 0.1371251,\n",
       " 0.01331838,\n",
       " 0.9750693,\n",
       " 0.021272317,\n",
       " 0.9752111,\n",
       " 0.97600627,\n",
       " 0.006898647,\n",
       " 0.009758932,\n",
       " 0.00857737,\n",
       " 0.017353231,\n",
       " 0.031976286,\n",
       " 0.008863671,\n",
       " 0.8050057,\n",
       " 0.4877995,\n",
       " 0.008904372,\n",
       " 0.9365789,\n",
       " 0.0072452123,\n",
       " 0.0297257,\n",
       " 0.9781822,\n",
       " 0.13407956,\n",
       " 0.012871934,\n",
       " 0.9732054,\n",
       " 0.9794932,\n",
       " 0.87306416,\n",
       " 0.012523912,\n",
       " 0.073708765,\n",
       " 0.9639591,\n",
       " 0.11423318,\n",
       " 0.009726264,\n",
       " 0.95768553,\n",
       " 0.011322761,\n",
       " 0.013255765,\n",
       " 0.25510404,\n",
       " 0.008303881,\n",
       " 0.96125567,\n",
       " 0.9785095,\n",
       " 0.0069002295,\n",
       " 0.01181167,\n",
       " 0.018572088,\n",
       " 0.9560054,\n",
       " 0.94432056,\n",
       " 0.008132881,\n",
       " 0.96070707,\n",
       " 0.010357232,\n",
       " 0.0064729583,\n",
       " 0.040236194,\n",
       " 0.007210083,\n",
       " 0.35793877,\n",
       " 0.016189458,\n",
       " 0.016052468,\n",
       " 0.97380406,\n",
       " 0.17696175,\n",
       " 0.015822757,\n",
       " 0.02815807,\n",
       " 0.009471986,\n",
       " 0.94311184,\n",
       " 0.9792661,\n",
       " 0.009773508,\n",
       " 0.01617587,\n",
       " 0.9087863,\n",
       " 0.0072654057,\n",
       " 0.27510092,\n",
       " 0.036988016,\n",
       " 0.84812206,\n",
       " 0.06417317,\n",
       " 0.010834523,\n",
       " 0.4536263,\n",
       " 0.011303233,\n",
       " 0.97523457,\n",
       " 0.00654831,\n",
       " 0.95682573,\n",
       " 0.58226246,\n",
       " 0.011405403,\n",
       " 0.014890175,\n",
       " 0.7406518,\n",
       " 0.95633924,\n",
       " 0.9689012,\n",
       " 0.017292205,\n",
       " 0.4063507,\n",
       " 0.02143488,\n",
       " 0.46534646,\n",
       " 0.03227973,\n",
       " 0.011608264,\n",
       " 0.9562013,\n",
       " 0.018366579,\n",
       " 0.0077046957,\n",
       " 0.4618462,\n",
       " 0.85010546,\n",
       " 0.008017219,\n",
       " 0.01779227,\n",
       " 0.9632363,\n",
       " 0.96920884,\n",
       " 0.03267208,\n",
       " 0.0067209466,\n",
       " 0.012304175,\n",
       " 0.94869435,\n",
       " 0.006362421,\n",
       " 0.58878976,\n",
       " 0.5241421,\n",
       " 0.9693853,\n",
       " 0.9736267,\n",
       " 0.01506653,\n",
       " 0.8374854,\n",
       " 0.01723991,\n",
       " 0.9580113,\n",
       " 0.03677514,\n",
       " 0.8182105,\n",
       " 0.12285632,\n",
       " 0.026803453,\n",
       " 0.039140884,\n",
       " 0.86664295,\n",
       " 0.04957694,\n",
       " 0.9745559,\n",
       " 0.9775828,\n",
       " 0.0116372,\n",
       " 0.007279707,\n",
       " 0.96874726,\n",
       " 0.011358847,\n",
       " 0.914155,\n",
       " 0.9283353,\n",
       " 0.9660704,\n",
       " 0.0068675918,\n",
       " 0.97150266,\n",
       " 0.07427638,\n",
       " 0.94558704,\n",
       " 0.97463113,\n",
       " 0.008194139,\n",
       " 0.011252929,\n",
       " 0.814282,\n",
       " 0.012328959,\n",
       " 0.0067524365,\n",
       " 0.01583531,\n",
       " 0.23607273,\n",
       " 0.95534736,\n",
       " 0.4491658,\n",
       " 0.008561295,\n",
       " 0.0077720294,\n",
       " 0.006744926,\n",
       " 0.023296406,\n",
       " 0.9548534,\n",
       " 0.049676027,\n",
       " 0.86923283,\n",
       " 0.96179307,\n",
       " 0.9536664,\n",
       " 0.18458584,\n",
       " 0.9639216,\n",
       " 0.9284643,\n",
       " 0.010967758,\n",
       " 0.017987086,\n",
       " 0.0070876675,\n",
       " 0.8960859,\n",
       " 0.2555844,\n",
       " 0.18705298,\n",
       " 0.9517109,\n",
       " 0.8641901,\n",
       " 0.057854217,\n",
       " 0.010275264,\n",
       " 0.512113,\n",
       " 0.011940375,\n",
       " 0.9699941,\n",
       " 0.012542474,\n",
       " 0.010973805,\n",
       " 0.97014713,\n",
       " 0.051229067,\n",
       " 0.58065623,\n",
       " 0.014270969,\n",
       " 0.008996185,\n",
       " 0.12517306,\n",
       " 0.008402546,\n",
       " 0.97035617,\n",
       " 0.19032979,\n",
       " 0.9704855,\n",
       " 0.012660354,\n",
       " 0.009508387,\n",
       " 0.9547713,\n",
       " 0.30177492,\n",
       " 0.9430938,\n",
       " 0.9798497,\n",
       " 0.010306945,\n",
       " 0.60624677,\n",
       " 0.010657179,\n",
       " 0.958733,\n",
       " 0.029812112,\n",
       " 0.009255899,\n",
       " 0.98074484,\n",
       " 0.06612627,\n",
       " 0.96811837,\n",
       " 0.37048244,\n",
       " 0.014646497,\n",
       " 0.9783829,\n",
       " 0.9679666,\n",
       " 0.5100332,\n",
       " 0.8904173,\n",
       " 0.010529825,\n",
       " 0.013057492,\n",
       " 0.006518549,\n",
       " 0.0076972046,\n",
       " 0.9466284,\n",
       " 0.010041255,\n",
       " 0.9502679,\n",
       " 0.92329675,\n",
       " 0.9190284,\n",
       " 0.96956015,\n",
       " 0.9768469,\n",
       " 0.9018612,\n",
       " 0.015010181,\n",
       " 0.013036383,\n",
       " 0.9569624,\n",
       " 0.114452064,\n",
       " 0.9667926,\n",
       " 0.9451123,\n",
       " 0.9718327,\n",
       " 0.9721416,\n",
       " 0.0073250015,\n",
       " 0.0071892478,\n",
       " 0.044653445,\n",
       " 0.757544,\n",
       " 0.019659325,\n",
       " 0.008188164,\n",
       " 0.9731333,\n",
       " 0.9732353,\n",
       " 0.01526922,\n",
       " 0.067575924,\n",
       " 0.40857565,\n",
       " 0.9644207,\n",
       " 0.02546756,\n",
       " 0.013153774,\n",
       " 0.009766556,\n",
       " 0.83983904,\n",
       " 0.9645292,\n",
       " 0.01257513,\n",
       " 0.017637165,\n",
       " 0.112035654,\n",
       " 0.97619027,\n",
       " 0.011445555,\n",
       " 0.044335507,\n",
       " 0.0070417454,\n",
       " 0.9668463,\n",
       " 0.006332245,\n",
       " 0.6113397,\n",
       " 0.009397571,\n",
       " 0.017999733,\n",
       " 0.95033544,\n",
       " 0.032284956,\n",
       " 0.0082757715,\n",
       " 0.96231294,\n",
       " 0.014983057,\n",
       " 0.015461457,\n",
       " 0.96260315,\n",
       " 0.009803763,\n",
       " 0.93300337,\n",
       " 0.013035438,\n",
       " 0.014234901,\n",
       " 0.9704555,\n",
       " 0.9756158,\n",
       " 0.94020176,\n",
       " 0.14758043,\n",
       " 0.011943194,\n",
       " 0.96392494,\n",
       " 0.9516072,\n",
       " 0.72323,\n",
       " 0.5497423,\n",
       " 0.8791208,\n",
       " 0.65792125,\n",
       " 0.014939033,\n",
       " 0.031372055,\n",
       " 0.9755265,\n",
       " 0.024578393,\n",
       " 0.0065084333,\n",
       " 0.048677914,\n",
       " 0.33015653,\n",
       " 0.92220235,\n",
       " 0.5801716,\n",
       " 0.8151108,\n",
       " 0.1016261,\n",
       " 0.032749314,\n",
       " 0.9126863,\n",
       " 0.018144004,\n",
       " 0.97716844,\n",
       " 0.049608447,\n",
       " 0.96874714,\n",
       " 0.010733391,\n",
       " 0.009090289,\n",
       " 0.009648139,\n",
       " 0.49613056,\n",
       " 0.0067354254,\n",
       " 0.1323988,\n",
       " 0.006320812,\n",
       " 0.034707714,\n",
       " 0.97726727,\n",
       " 0.043005824,\n",
       " 0.66910744,\n",
       " 0.97405845,\n",
       " 0.067059904,\n",
       " 0.010523556,\n",
       " 0.09540579,\n",
       " 0.014905299,\n",
       " 0.0114637,\n",
       " 0.96865183,\n",
       " 0.9746207,\n",
       " 0.67390656,\n",
       " 0.010408519,\n",
       " 0.9706435,\n",
       " 0.009744893,\n",
       " 0.20746893,\n",
       " 0.0079500275,\n",
       " 0.25059137,\n",
       " 0.83180165,\n",
       " 0.016821286,\n",
       " 0.011755784,\n",
       " 0.97778213,\n",
       " 0.96883833,\n",
       " 0.007998127,\n",
       " 0.012495458,\n",
       " 0.0069973315,\n",
       " 0.9669827,\n",
       " 0.037070118,\n",
       " 0.0110598365,\n",
       " 0.009735377,\n",
       " 0.48570052,\n",
       " 0.96871626,\n",
       " 0.21759003,\n",
       " 0.94773394,\n",
       " 0.012287834,\n",
       " 0.9802988,\n",
       " 0.015840694,\n",
       " 0.011465231,\n",
       " 0.28133956,\n",
       " 0.010826409,\n",
       " 0.06267471,\n",
       " 0.008657592,\n",
       " 0.006555924,\n",
       " 0.966406,\n",
       " 0.008063751,\n",
       " 0.009643958,\n",
       " 0.016834518,\n",
       " 0.97739697,\n",
       " 0.011410385,\n",
       " 0.9776375,\n",
       " 0.008845516,\n",
       " 0.017860476,\n",
       " 0.96316373,\n",
       " 0.93442935,\n",
       " 0.26344335,\n",
       " 0.010587468,\n",
       " 0.010259411,\n",
       " 0.019287087,\n",
       " 0.96094424,\n",
       " 0.012278276,\n",
       " 0.0063238237,\n",
       " 0.96847534,\n",
       " 0.013457279,\n",
       " 0.007975398,\n",
       " 0.014649451,\n",
       " 0.97147286,\n",
       " 0.96381974,\n",
       " 0.014350932,\n",
       " 0.009327071,\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./y_hat_clinical_noN.npy',y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008936026\n"
     ]
    }
   ],
   "source": [
    "print(y_proba[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[434  47]\n",
      " [ 26 215]]\n",
      "{'max_f1': 0.8548707753479126, 'precision': 0.8206106870229007, 'recall': 0.8921161825726142, 'thres': 0.64}\n"
     ]
    }
   ],
   "source": [
    "optimal_point,f1_list = utils.f1_maximize(y_hat[2887:],y_list[2887:])\n",
    "print(optimal_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_f1': 0.9016328156113101,\n",
       " 'precision': 0.8654434250764526,\n",
       " 'recall': 0.940980881130507,\n",
       " 'thres': 0.68}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "statis_dict = {'false_positive':[],'false_negative':[]}\n",
    "y_pred = np.array(y_hat) > 0.68\n",
    "statis_dict =  utils.wrong_statis(y_list,y_pred,statis_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "statis_dict_test = {'false_positive':[],'false_negative':[]}\n",
    "y_pred = np.array(y_hat) > 0.68\n",
    "statis_dict_test =  utils.wrong_statis(y_list[2887:],y_pred[2887:],statis_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'false_positive': [array([  8,  21,  38,  44,  50,  60,  71,  73,  95, 112, 127, 137, 151,\n",
       "         154, 163, 165, 221, 246, 270, 275, 302, 311, 329, 335, 349, 393,\n",
       "         395, 449, 461, 489, 513, 522, 523, 552, 561, 592, 622, 625, 655,\n",
       "         662, 666, 675, 705, 712])],\n",
       " 'false_negative': [array([ 26,  27,  62,  70,  75,  88, 158, 173, 223, 226, 285, 292, 298,\n",
       "         314, 385, 415, 419, 439, 444, 445, 451, 471, 475, 503, 507, 529,\n",
       "         541, 562, 629, 649, 679, 699])]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statis_dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(statis_dict['false_positive'][0]) + len(statis_dict['false_negative'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "src_dir = '../preprocessing/result/ratio2/'\n",
    "label_train = pickle.load(open(src_dir+'label_train.pkl','rb'))    \n",
    "label_test = pickle.load(open(src_dir+'label_test.pkl','rb'))\n",
    "label_train.extend(label_test)\n",
    "y_list = label_train\n",
    "y_hat = np.load('./y_hat_clinical_noN.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9588340335228301\n"
     ]
    }
   ],
   "source": [
    "roc_score = sklearn.metrics.roc_auc_score(y_list[2887:], y_hat[2887:])\n",
    "print(roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "y_list = np.array(y_list[2887:])\n",
    "y_hat  = np.array(y_hat[2887:])\n",
    "diff = y_list - y_hat\n",
    "obivious_wrong = np.where(diff < -0.8)[0]\n",
    "print(len(obivious_wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label accroding to ICD is 0, but model predicted: 0.9451741576194763\n",
      "The label accroding to ICD is 0, but model predicted: 0.9024368524551392\n",
      "The label accroding to ICD is 0, but model predicted: 0.9695841073989868\n",
      "The label accroding to ICD is 0, but model predicted: 0.9618653059005737\n",
      "The label accroding to ICD is 0, but model predicted: 0.9324651956558228\n",
      "The label accroding to ICD is 0, but model predicted: 0.8170803785324097\n",
      "The label accroding to ICD is 0, but model predicted: 0.9519528746604919\n",
      "The label accroding to ICD is 0, but model predicted: 0.8828083872795105\n",
      "The label accroding to ICD is 0, but model predicted: 0.9075936675071716\n",
      "The label accroding to ICD is 0, but model predicted: 0.8801155090332031\n",
      "The label accroding to ICD is 0, but model predicted: 0.876316249370575\n",
      "The label accroding to ICD is 0, but model predicted: 0.904143214225769\n",
      "The label accroding to ICD is 0, but model predicted: 0.9385735392570496\n",
      "The label accroding to ICD is 0, but model predicted: 0.9597023725509644\n",
      "The label accroding to ICD is 0, but model predicted: 0.9456961750984192\n",
      "The label accroding to ICD is 0, but model predicted: 0.9473834037780762\n",
      "The label accroding to ICD is 0, but model predicted: 0.919357419013977\n",
      "The label accroding to ICD is 0, but model predicted: 0.9232742190361023\n",
      "The label accroding to ICD is 0, but model predicted: 0.9326470494270325\n",
      "The label accroding to ICD is 0, but model predicted: 0.9081906080245972\n",
      "The label accroding to ICD is 0, but model predicted: 0.9731289744377136\n",
      "The label accroding to ICD is 0, but model predicted: 0.8302387595176697\n",
      "The label accroding to ICD is 0, but model predicted: 0.9498255848884583\n",
      "The label accroding to ICD is 0, but model predicted: 0.8601991534233093\n",
      "The label accroding to ICD is 0, but model predicted: 0.8331359028816223\n",
      "The label accroding to ICD is 0, but model predicted: 0.8316851854324341\n",
      "The label accroding to ICD is 0, but model predicted: 0.9128972291946411\n",
      "The label accroding to ICD is 0, but model predicted: 0.9715035557746887\n",
      "The label accroding to ICD is 0, but model predicted: 0.9446427226066589\n",
      "The label accroding to ICD is 0, but model predicted: 0.9586939811706543\n",
      "The label accroding to ICD is 0, but model predicted: 0.9705618619918823\n",
      "The label accroding to ICD is 0, but model predicted: 0.893028974533081\n"
     ]
    }
   ],
   "source": [
    "log = 'The label accroding to ICD is {}, but model predicted: {}'\n",
    "for sample_index in obivious_wrong:\n",
    "    print(log.format(y_list[sample_index], y_hat[sample_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 38,  44,  50,  60,  71, 127, 137, 154, 163, 165, 246, 270, 302,\n",
       "       311, 329, 393, 395, 449, 461, 489, 513, 522, 523, 552, 561, 592,\n",
       "       622, 625, 662, 666, 705, 712])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obivious_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  15,   33,  164,  174,  239,  256,  300,  326,  362,  366,  410,\n",
       "        463,  712,  741,  760,  816,  843,  849,  900, 1175, 1190, 1252,\n",
       "       1263, 1265, 1377, 1448, 1495, 1502, 1515, 1608, 1651, 1696, 1703,\n",
       "       1747, 1756, 1800, 1867, 1909, 1988, 1996, 2025, 2112, 2139, 2233,\n",
       "       2333, 2344, 2385, 2419, 2461, 2509, 2514, 2516, 2525, 2575, 2647,\n",
       "       2738, 2794, 2846, 2925, 2931, 2937, 2947, 2958, 3024, 3050, 3157,\n",
       "       3189, 3198, 3216, 3280, 3282, 3336, 3348, 3376, 3400, 3410, 3509,\n",
       "       3512, 3549, 3553, 3592])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obivious_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537\n",
      "642\n",
      "652\n",
      "919\n",
      "1348\n",
      "1549\n",
      "1600\n",
      "1675\n",
      "1699\n",
      "1733\n",
      "1768\n",
      "1913\n",
      "1991\n",
      "2208\n",
      "2474\n",
      "2486\n",
      "2709\n",
      "2975\n",
      "3060\n",
      "3113\n",
      "3172\n",
      "3179\n",
      "3201\n",
      "3302\n",
      "3331\n",
      "3358\n",
      "3449\n",
      "3536\n"
     ]
    }
   ],
   "source": [
    "for a in obivious_wrong:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
